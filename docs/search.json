[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "This is where I’ll put projects from Data Science 2!\n\n\n\n\n\n\n    \n      \n      \n    \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "hw.html",
    "href": "hw.html",
    "title": "HW Assignments",
    "section": "",
    "text": "Maybe I’ll put homework in here–who knows?\n\n\n\n\n\n\n\n    \n      \n      \n    \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mary Kreklow",
    "section": "",
    "text": "Welcome!\nI’m Mary Kreklow, a junior at St. Olaf College, studying quantitative economics with concentrations in statistics & data science and international relations. My academic journey is driven by a commitment to understanding complex systems—whether that’s through data analysis, policy evaluation, or modeling solutions for real-world issues. Outside the classroom, I lead as a SOAR Peer Leader and Senior Admissions Fellow, and I’m actively involved in music ensembles and campus organizations. With a passion for helping others and finding good in the world, I’m excited to work on projects that reflect my dedication to bridging gaps and uplifting communities.\nHere, you’ll find a collection of projects that showcase my curiosity and creativity, offering a playful take on data-driven insights!"
  },
  {
    "objectID": "index.html#mary-kreklow",
    "href": "index.html#mary-kreklow",
    "title": "Mary Kreklow",
    "section": "",
    "text": "Welcome!\nI’m Mary Kreklow, a junior at St. Olaf College, studying quantitative economics with concentrations in statistics & data science and international relations. My academic journey is driven by a commitment to understanding complex systems—whether that’s through data analysis, policy evaluation, or modeling solutions for real-world issues. Outside the classroom, I lead as a SOAR Peer Leader and Senior Admissions Fellow, and I’m actively involved in music ensembles and campus organizations. With a passion for helping others and finding good in the world, I’m excited to work on projects that reflect my dedication to bridging gaps and uplifting communities.\nHere, you’ll find a collection of projects that showcase my curiosity and creativity, offering a playful take on data-driven insights!"
  },
  {
    "objectID": "us_states.html",
    "href": "us_states.html",
    "title": "US States",
    "section": "",
    "text": "Rat Graph\nThis graph shows the proportion of rats per person for each state. One might use this if they have either a strong like or dislike for rats and are willing to relocate accordingly. Someone who really loves rats and would like to cohabitate with them may wish to check out New York, as we see that there is about a rat for each person residing there. On the other hand, someone who is looking to stay as far away as possible might have better luck with somewhere like Colorado, where they will find closer to one rat for every ten people.",
    "crumbs": [
      "US States"
    ]
  },
  {
    "objectID": "wi_gerrymandering.html",
    "href": "wi_gerrymandering.html",
    "title": "Wisconsin Gerrymandering",
    "section": "",
    "text": "2016 WI Congressional Districts\nThis map shows the congressional Districts in WI as of 2016. The colors from blue to red represent how respectively Democratic or Republican that district voted in the 2016 elections for Congress.\nOne interesting aspect is the shapes of the districts. We see that some districts, like Districts 3 & 4 are very oddly shaped. District 3 seems to reach up into areas like Stevens Point and Eau Claire, while District 4 is by far the smallest, and covers little more than Milwaukee. This could bring up accusations of gerrymandering, especially seeing as the two weirdest shaped districts are also 2 of the 3 that elected Democratic Representatives. If, for example, District 4 was a bit bigger, it might draw some of the republican votes away from their respective districts and make those districts more competitive and at risk of turning blue. The size of District 7 is also notable. It is likely that there are some more liberal areas in NW Wisconsin around Superior, however their votes would be cancelled out by the large span of conservative areas in northcentral WI.\nWe also see that the darkness of the color corresponds to the proportion of votes. For example, districts 3 & 4 have no Republican votes because they were uncontested competitions. The majority of the Republican winning counties had around 60-65% of the vote.\nWI Governor Evers recently signed a bill passing a new Legislative Districts map to combat some of this gerrymandering, however thus far the Congressional Districts have remained the same.",
    "crumbs": [
      "Wisconsin Gerrymandering"
    ]
  },
  {
    "objectID": "rtip.html",
    "href": "rtip.html",
    "title": "R Tip of the Day",
    "section": "",
    "text": "Circlize Package\nThe circlize package is very helpful in displaying data circularly. This can allow us to stack different types of charts about the same data, as shown below, and it also has many benefits pertaining to looking at connections between variables.\n\n\n\n\n\n\n\n\n\nThis next chart is just an example of how you can facet these charts. You can, of course, have multiple circles that all display data and charts, however for this example I choose to just play with their colors and brightness.\n\n\n\n\n\n\n\n\n\nWhile the first two graphs were just playing around with the package with random variables, this last graph is a real world example of how the circlize package might be used. The World Phones dataset shows the amount of phones in different regions of the world throughout a few years. I am using it with the chordDiagram function to show where there were phones in the regions throughout the years.\n# Load circlize package\nlibrary(circlize)\n\n# Load WorldPhones dataset\ndata(\"WorldPhones\")\n\n# Create a circular plot\nchordDiagram(WorldPhones, transparency = 0.5)\n\n\n\n\n\n\n\n\n\nFor more information, this link will take you to a presentation with an overview of the circlize dataset and some of its uses.",
    "crumbs": [
      "R Tip of the Day"
    ]
  },
  {
    "objectID": "mini_project2.html",
    "href": "mini_project2.html",
    "title": "Simulation",
    "section": "",
    "text": "Fat Rats!\n\nFor this project, I examined the dataset FatRats, which has data from an experiment on the effects of diet on the weight gain of rats. The study had 60 baby rat participants, which were split up into groups and fed different sources of protein (Beef, Cereal, and Pork). The resulting dataset shows the weight gain for each rat from their protein source.\nI decided to conduct a randomization-type test by simulating the behavior under the null hypothesis. I wanted to see if the results of weight gain from different protein sources is statistically significant. I did this by running simulations of the study and randomizing the protein source to see if a random test would produce significantly different responses than the experiment did.\nThe following graphs show the difference in average weight gains between different proteins with the randomized test, which shows what might happen if the type of protein had no effect. The hot pink vertical line shows the average weight gain from the study that was run. The p-values below each chart show the probability under the null hypothesis (our randomized experiment) of obtaining a test statistic at least as extreme as the one obtained. The p-values are calculated by looking at how many of the randomized statistics are greater than or equal to the observed statistics from the rat study.\n\n\n\n\n\n\n\n\n\nBeef vs. Pork p-value: 0.275 \n\n\n\n\n\n\n\n\n\n\n\nBeef vs. Cereal p-value: 0.296 \n\n\n\n\n\n\n\n\n\n\n\nPork vs. Cereal p-value: 0.291 \n\n\nAs we can see, all of the p-values are close to 0.3. Generally, if the p-value is less than 0.05, we will consider the results of the study statistically significant. Because these results are well over the point of statistical significance, we can fail to reject the null hypothesis and claim that the difference in weight gains due to different proteins is not statistically significant.",
    "crumbs": [
      "Simulation"
    ]
  },
  {
    "objectID": "RATATOUILLE.html",
    "href": "RATATOUILLE.html",
    "title": "Strings & Regular Expressions",
    "section": "",
    "text": "Ratatouille\nDid you know that in the Ratatouille movie, only 7,526 words were spoken? According to a Reddit post, the average screenplay has about 22,000 words (although Reddit is definitely not the most reliable source so this may not be totally accurate). Within those 7,526 words in Ratatouille, there were 2,193 distinct words spoken. This may seem like very few, but any Ratatouille lover knows how impactful those 2,193 different words were.\n\n\n\n\n\n\n\n\n\nThe above graph is a sentiment analysis of all of the words in Ratatouille. To find this, I used a sentiment library that ranks common words on a scale of -3 to 3, respectively negative to positive sentiment. I inner joined that dataset with my dataset of Ratatouille words to find their sentiments. We can see that a little over half of the Ratatouille words have positive connotations. This makes sense since it is overall a happy movie, although there is a lot of yelling from the head chef which would contribute to the good chunk of negativity. This graph has the words in order of appearance, so the left is the beginning of the movie, and as the movie progresses, we move to the right. We can see areas where the movie was more positive, and also areas that were more negative.\nBelow we see a table with the 10 most used words in Ratatouille. To do this, I removed the stop words (common words that generally don’t hold significance like “the”) from the data and I ranked the leftover words by amount used. Unsurprisingly, we see that 3 of the words have to do with food, as it is a food movie. We also see that names/nicknames come up a lot, with Remy, Gusteau, Linguini and dad all making appearances. Rat is also fairly expected since the whole plot of the movie is about a rat. Somewhat surprising, however, is the word hey. I would think that it would be considered a stop word, however the set of stop words I used did not include it.\n\n\n\n\n\n\nword\nchef\ngusteau\nrat\ncook\ndad\nfood\nhey\nlinguini\nremy\nwait\n\n\nn\n55\n40\n33\n28\n26\n26\n23\n21\n21\n18\n\n\n\n\n\n\n\n\nWhile seeing these words is nice to get a taste for what the most used words are, it is also nice to have a more visual representation. Below is a word cloud containing the top 100 used words from the move. The bigger the word, the more it was used. By using a diagram like this, we can more easily see which words are the most commonly found, and the color codings show up what other words were used a similar amount of times.\n\n\n\n\n\n\n\n\n\nNow that we know which words were used the most in Ratatouille, we can look into where in the movie these words show up. For the next graph, I looked into the top 5 words and where they are positioned in comparison to all other words in the movie. We can see that “dad” was used mostly in 3 separate areas of the movie, whereas “chef” was used pretty regularly throughout.\n\n\n\n\n\n\n\n\n\nAnyone who has watched Ratatouille surely understands that it is a masterpiece, but what do people really think about the film? To find this out, we can look to the master site of movie reviews, Rotten Tomatoes. On Rotten Tomatoes, Ratatouille scores a 96% on the Tomatometer (critic reviews) and an 87% for audience ratings. It is clear that the critics are the true experts in what makes a good movie! The graph below shows a sentiment analysis of words from all reviews (critic and audience).\n\n\n\n\n\n\n\n\n\nWe say that Ratatouille has, by far, had a positive impact on watchers. However, it leaves one to wonder what some of the negative reviews had to say about the movie. The negatively connotated words in these reviews are “cry”, “bored”, “hard”, “struggling”, “critic”, and “leave”. Many of these words are not inherently negative, and would depend on the context.\n\n\n\n\n\n\nword\nreviews\n\n\n\n\ncry\nlike the snooty restaurant &lt;critic&gt; anton ego (voiced by peter o'toole), we enter ratatouille thinking we've tasted it all, and we leave hoping for seconds.\n\n\nbored\nlike the snooty restaurant critic anton ego (voiced by peter o'toole), we enter ratatouille thinking we've tasted it all, and we &lt;leave&gt; hoping for seconds.\n\n\nhard\nlike the snooty restaurant &lt;critic&gt; anton ego (voiced by peter o'toole), we enter ratatouille thinking we've tasted it all, and we leave hoping for seconds.\n\n\nstruggling\nlike the snooty restaurant critic anton ego (voiced by peter o'toole), we enter ratatouille thinking we've tasted it all, and we &lt;leave&gt; hoping for seconds.\n\n\ncritic\nlike the snooty restaurant &lt;critic&gt; anton ego (voiced by peter o'toole), we enter ratatouille thinking we've tasted it all, and we leave hoping for seconds.\n\n\nleave\nlike the snooty restaurant critic anton ego (voiced by peter o'toole), we enter ratatouille thinking we've tasted it all, and we &lt;leave&gt; hoping for seconds.\n\n\n\n\n\n\n\n\nAs we can see, none of these words that are considered to have negative connotations are actually being used in a negative sense. On the contrary, they are actually all part of very positive reviews about the movie.\nNow that we know quite a bit about the contents of Ratatouille, we can look at how some of its statistics compared to other Disney Movies. I know that for a lot of people, time is valuable, so I wanted to see which movies are conserving your time and which are taking more of it. For the first comparison, I found a dataset of many Disney movies and looked at the lengths of the movies.\n\n\n\n\n\n\n\n\n\nWe can see that Ratatouille is just slightly over the average running time, with 111 minutes. The mean running time for the Disney movies in my dataset is 97.51009, with a standard deviation of 18.11338. This means the Ratatouille is within one standard deviation of the mean and is pretty average as far as movie length.\nOne part of Ratatouille that may be less average is its box office statistics. Ratatouille was, and continues to be, very popular, so unsurprisingly it did very well and has a very high box office score of 620.7 million dollars.\n\n\n\n\n\n\n\n\n\nThe average box office for the Disney movies is $169.8 million, however there is a $277.4 million standard deviation, so there is definitely a very wide range. Nonetheless, Ratatouille is still very high on the list and did very well for a Disney movie.\nI was also interested in looking into how the box office scores of Disney movies compared with those of a different company’s movies. I found a dataset of Warner Bros. movies and their box scores to look into the difference between their means.\n\n\n\n\n\n\n\n\n\nWe can see that Warner Bros has more lower revenue movies, while Disney seems to have more higher revenue films. Because we cannot make definite conclusions from this graph, I performed a two-sample t-test to determine if there is a significant difference in the mean box office earnings of Disney movies and Warner Bros. movies.\n\n\n\n    Welch Two Sample t-test\n\ndata:  disney_movies$`Box office (float)` and warner_bros_movies$`Worldwide\\nBox Office`\nt = 3.6751, df = 462.46, p-value = 0.0002656\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 27441671 90514195\nsample estimates:\nmean of x mean of y \n169826342 110848408 \n\n\nWe can see that the results of this t-test find a significant difference in the means, with a p-value in the ten-thousandths. It shows much higher average box office earnings for the Disney movies than the Warner Bros movies, however there are several factors that could be affecting this. Firstly, my dataset for Warner Bros movies is much larger, meaning that it probably includes many smaller films that may have been small outliers and dragged the means down. Furthermore, I also got my dataset for the Disney movies from Kieth Galli’s github and do not have information about how the films were chosen because it is not comprehensive of all Disney movies. One thing that could help find a more accurate result by making the datasets the same size and using the Warner Bros films that have the largest box office earnings.\n\n\n\n    Welch Two Sample t-test\n\ndata:  disney_movies$`Box office (float)` and new_warner$`Worldwide\\nBox Office`\nt = -4.8361, df = 672.97, p-value = 1.643e-06\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -132475481  -55966022\nsample estimates:\nmean of x mean of y \n169826342 264047093 \n\n\nComparing the Disney movies to only the top Warner Bros movies just shows a larger gap, meaning that in reality, Disney movies have had much higher box office earnings and we can reject the null hypothesis and claim a statistical significance between the means. Yay Disney!\nAfter seeing how successful these Disney movies were, we can see if there is any correlation between the amount spent on the movie and how well it did. To do this, I made a scatterplot looking at the relationship between budget and box office earnings.\n\n\n\n\n\n\n\n\n\nWe can see a positive relationship, meaning that the movies with higher budgets tended to do better when they were released. The light pink and slightly larger point represents Ratatouille, so we can see that Ratatouille was very close to the regression line and was following the trend pretty closely.\nKnowing this, I was curious to see what the profits of movies looked like. I made a new variable that subtracted the budgets from the box office earnings to find the profit. I then put the 30 top movies by profit in a table.\n\n\n\n\n\n\ntitle\nBox office (millions)\nBudget (millions)\nProfit (millions)\n\n\n\n\nThe Lion King\n1657.0\n250.0\n1407.0\n\n\nFrozen II\n1450.0\n150.0\n1300.0\n\n\nFrozen\n1280.0\n150.0\n1130.0\n\n\nBeauty and the Beast\n1264.0\n160.0\n1104.0\n\n\nIncredibles 2\n1243.0\n200.0\n1043.0\n\n\nThe Lion King\n968.5\n45.0\n923.5\n\n\nZootopia\n1024.0\n150.0\n874.0\n\n\nToy Story 4\n1073.0\n200.0\n873.0\n\n\nToy Story 3\n1067.0\n200.0\n867.0\n\n\nFinding Dory\n1029.0\n175.0\n854.0\n\n\nFinding Nemo\n940.3\n94.0\n846.3\n\n\nPirates of the Caribbean: Dead Man's Chest\n1066.0\n225.0\n841.0\n\n\nAlice in Wonderland\n1025.0\n200.0\n825.0\n\n\nThe Jungle Book\n966.6\n175.0\n791.6\n\n\nInside Out\n858.0\n175.0\n683.0\n\n\nPirates of the Caribbean: At World's End\n961.0\n300.0\n661.0\n\n\nPirates of the Caribbean: On Stranger Tides\n1046.0\n410.6\n635.4\n\n\nCoco\n807.1\n175.0\n632.1\n\n\nMaleficent\n758.5\n180.0\n578.5\n\n\nThe Chronicles of Narnia: The Lion, the Witch and the Wardrobe\n745.0\n180.0\n565.0\n\n\nPirates of the Caribbean: Dead Men Tell No Tales\n794.9\n230.0\n564.9\n\n\nUp\n735.1\n175.0\n560.1\n\n\nMonsters University\n743.6\n200.0\n543.6\n\n\nThe Incredibles\n633.0\n92.0\n541.0\n\n\nMoana\n690.8\n150.0\n540.8\n\n\nPirates of the Caribbean: The Curse of the Black Pearl\n654.3\n140.0\n514.3\n\n\nBig Hero 6\n657.8\n165.0\n492.8\n\n\nAladdin\n504.1\n28.0\n476.1\n\n\nRatatouille\n620.7\n150.0\n470.7\n\n\nMonsters, Inc.\n577.4\n115.0\n462.4\n\n\n\n\n\n\n\n\nIn this table, Ratatouille takes 29th place as most profitable. Considering this is a dataset of 347 movies, that is quite impressive. It is also worth noting the $470.7 million profit – the masterminds behind Ratatouille must be living well.\nI hope you have enjoyed coming on this data journey with me, and I hope you learned something new (or many new things) about Ratatouille. Now that you have made it to the end, I think that you, dear reader, should have a seat, grab a snack, relax, and enjoy a wonderful screening of Ratatouille!",
    "crumbs": [
      "Strings & Regular Expressions"
    ]
  },
  {
    "objectID": "FinalProject.html",
    "href": "FinalProject.html",
    "title": "Final Project",
    "section": "",
    "text": "Hide-and-SQUEAK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHide-and-SQUEAK\n\n\nLiesl Eckstrom, Maia Chavez, Mary Kreklow\n\n\n12/18/2023\n\n\n\nWant to learn some RAT-ical information to RAT-tle your brain? You have come to the right place! Here, you can learn all about rats in New York City - we will keep no squeak-rets from you! If you have musophobia (a fear of rats) feel free to exit, otherwise join us on our journey as we share and interp-RAT some of our favo-RAT stats. And what better way to showcase this data than in elaboRATe shades of pink?\n\n\n\nIntro to The Rats Dataset\n\n\nFor this project, we focused on using a dataset called “Rat Sightings,” which is from NYC Open Data. This data has service requests of right sightings in New York City from 2010 to the present (Dec. 2023). It offers information about the location of the sighting, including which of New York City’s boroughs, as well as the date and time of the sighting.\n\n\nWe also used a dataset with weather information from NYC. This dataset was scraped from Weather Underground and includes average daily temperatures, daily high and low temperatures, as well as weather events and precipitation details for every day from 1940-2020. We used the data from 2010 to 2020, as these are the dates that overlap with our Rat Sightings dataset.\n\n\nWe will use these data to discover where and when rats are most abundant!\n\n\n\n\nMap\n\n\nMost people are very pro-rat or anti-rat, so they would either be happy to see rats or avoid them at all costs. Because of this, one of the first things that comes to mind when you think of rats is wondering where you are most likely, or least likely, to find them. We decided that the best way to explore this further would be to look at the areas of New York City with the highest concentrations of rats. We decided to make a heat map to help visualization of the most rat-packed areas.\n\n\n\n\n\nAs shown, this is a heat map that shows the areas of New York City with the observations of rat sightings. We decided to use a map of the different boroughs and plot the rat sightings by their longitude and latitude. As shown by the darker shades of pink on the map, the two boroughs with the highest concentration of rat sightings are Brooklyn and Manhattan, followed by Bronx and Queens. If you are a rat-hater, we would recommend sticking close to Staten Island, where there are less rats to be found. However, if you are looking to make some rat friends (perhaps a bro-dent or two), you would probably have the best luck in Brooklyn or Manhattan.\n\n\n\n\nMost Popular Rat Locations\n\n\nIn order to further examine where rats are more abundant, we looked at what types of locations the rats are most commonly sighted. We split the variable Location type into seven categories: residential, other, non residential buildings, outdoor locations, vacant locations, educational locations, and food locations. We decided to use a bar graph to visualize where rats are most commonly seen.\n\n\n\n\n\nIn this bar graph, you can see that most rat sightings occur in residential areas. It’s surprising that there were way more rat sightings in residential areas than there were in outdoor sites, which include places like sewers and parking lots. While this is unsettling, it’s also comforting to know that very few rat sightings occur in food locations such as grocery stores and restaurants. There also weren’t many rat sightings in educational areas. The second most common type of location to see rats was “Other”, which we weren’t given much information about in the dataset, so it is unclear where a lot of rat sightings happen. We can also see from the bar chart that rat sightings in these locations have generally gone up as the years have gone by.\n\n\n\n\nPopular Residential Streets\n\n\nNow that we know that residential areas have the most rat sightings, we can determine which residential streets are most popular with rats. To this we decided to filter the data set to only include residential locations, find the five most frequent streets, and use a faceted scatterplot with the year to see how many rats visit these popular streets each year.\n\n\n\n\n\nThe five most popular residential streets for the rats to hang out in are Bedford Avenue, Broadway, Eastern Parkway, Grand Concourse, and St. Johns Place. So, if you are ever looking to settle down with your rat friends in New York, get a home on these streets. Some of these locations have had an increase in rat friends over the years, but others have a pretty consistent population. As you can see above, St. Johns Place, Bedford Avenue, and Broadway have grown in popularity. St. Johns Place increases a lot, while our Bedford Avenue and Broadway rats increase very slightly. Grand Concourse and Eastern Parkway have been pretty consistent with their number of rat sightings. However, Eastern Parkway did have a very large amount of rat sightings for a couple of years.\n\n\n\n\nRat Sightings in Park Boroughs\n\n\nThe next variables that we looked at with this data set were park borough and month. We made a bar chart with months and rat sightings on the axes to show the spread over the months of the years and used a fill with park borough to show the spread over park borough.\n\n\n\n\n\nThere was a significant trend for more rat sightings in the warmer months than the colder months with a peak in July and steadily decreasing both ways from the peak. We think that this could be for two different reasons, either there are more people out and about to report the rat sightings or there are more rats out and about to be spotted when it’s warmer outside. The park borough with the most amount of rat sightings is Brooklyn and the one with the least amount of sightings is Staten Island. Our hypothesis for this is that Brooklyn has the most rat sightings because it has the highest population and that Staten Island has the least amount of rat sightings for two different reasons. The reasons are that Staten Island has the smallest population and it is more separated from the rest of the New York City park boroughs.\n\n\n\n\nRats and Weather Conditions\n\n\nOne of our other curiosities was if the day’s weather conditions had an effect on the number of rat sightings that were reported. Because our rats dataset didn’t include weather information, we had to add a dataset with weather data to our rats dataset. We joined the datasets together and were able to look at both weather events and daily temperatures. We chose to use the average daily temperature as well as the occurrence of weather events for this graph. We hypothesized that rats would be seen the most on warmer (70s) days with no weather events because that is the time when we most prefer to be out and about and figured the rats might agree.\n\n\n\n\n\nWe were correct in our temperature hypothesis, as we can see that the highest number of rat observations was at temperatures in the upper 70 degrees Fahrenheit. However, we were slightly off with our observation that most of the observations would be on days with no weather events, as it appears that close to half of the observations were made on days with weather events, the main one being rain. This could be for a few reasons, including the fact that most weather events do not last an entire day. Even if a weather event was recorded for the same day as the rat sighting, that does not mean that the rat sighting happened during the weather event. Because we do not have weather observations directly associated with the rat sightings, it is impossible to truly know the distribution. Nonetheless, you do have a greater chance of seeing a rat when there is not a weather event, therefore if you want to avoid rats, you will have a better chance if you go out on miserable days full of snow, thunderstorms, and fog.\n\n\n\n\nLimitations\n\n\nThe largest limitation for our project is that the dataset we used for rat sightings can only track the rats that people are seeing and not all the rats in New York City, so this data is not going to be representative of all the rats in NYC.\n\n\nIt would also be interesting to look into how rat patterns in New York City compare to other cities, and different programs the city might have in place to control the rat population.\n\n\n\n\nConclusion\n\n\nWe hope these rat graphs were not too cheesy for you. Perhaps learning about rats has made you squeak with delight! Either way, we feel that this information is seriously under-rat-ed, and would like to cong-rat-ulate you for making it through. Have a mice day!"
  },
  {
    "objectID": "SDS272.html",
    "href": "SDS272.html",
    "title": "Liesl & Mary EDA",
    "section": "",
    "text": "##Storyboard"
  },
  {
    "objectID": "SDS272.html#storyboard",
    "href": "SDS272.html#storyboard",
    "title": "McStats: Serving Up the Data on Fast Food Nutrition",
    "section": "Storyboard",
    "text": "Storyboard"
  },
  {
    "objectID": "SDS272.html#introductory-graphs",
    "href": "SDS272.html#introductory-graphs",
    "title": "McStats: Serving Up the Data on Fast Food Nutrition",
    "section": "Introductory Graphs",
    "text": "Introductory Graphs"
  },
  {
    "objectID": "SDS272.html#intro",
    "href": "SDS272.html#intro",
    "title": "McStats: Serving Up the Data on Fast Food Nutrition",
    "section": "Intro",
    "text": "Intro\nThe amount of calories in the food that we eat is important. Eating over your maintenance amount of calories can alter the activity of the HPA-axis, which produces hormones, causing our bodies to have higher circulating levels of the stress hormone cortisol (George et al., 2009). Chronic overproduction of cortisol can have many adverse health effects on humans including obesity. Obesity is a rising public health issue, especially in the United States, as the occurrence rates increase (Overweight & Obesity Statistics - Niddk, n.d.). Researching obesity is important because it has high rates of comorbidity with many other very serious health issues including type 2 diabetes, hypertension, sleep apnea, arthritis, and certain types of cancer (Pi-Sunyer, 1999). A factor that can contribute to rising rates of obesity is easy access to affordable, fast, and calorie dense foods.\nAwareness of the caloric content of our food is one important way that we can make informed decisions about the meals that we eat. Fast food chains now, by law, must display the calorie count on each of their items and have nutritional information on hand if requested, but this might not be enough. Block et al., showed that people still underestimate the amount of calories in the fast food meals that they eat, especially when dining at Subway compared to McDonald’s (2013).\nThough eating the right amount of calories is important, it is also not the whole picture. Understanding the nutritional content of the food we eat, such as, levels of protein, carbohydrate, sugar, fat, etc. is invaluable to consider, to make sure that our bodies have all the nutrients they need to function. Knowing these values may also help us make predictions about the amount of calories in our food.\nIn our research we explored different nutritional factors and their relationship with the number of calories, amount of protein, and amount of carbohydrates in fast food meals. We also explored how different fast food chains compare in the amount of calories that are in their products. We hypothesize that there are significant differences between average calorie amounts at different restaurants and that there is a significant linear relationship between our response and explanatory variables."
  },
  {
    "objectID": "SDS272.html#eda",
    "href": "SDS272.html#eda",
    "title": "McStats: Serving Up the Data on Fast Food Nutrition",
    "section": "EDA",
    "text": "EDA\nFirstly, we checked the residuals and conditions of our data to determine if it would be appropriate for modeling because we wanted to run linear regressions. All of our response variables were generally unimodel and roughly normally distributed. The relationship between our response and explanatory variables was linear, as seen in figs. 7-9 in the appendix. We also found equal variance and normalcy in the residuals, aside from some expected high outliers. Because each observation is a menu item, we do not ancipate any issues with independence.\n\n\n\n\n\nThis graph shows the distribution of calories across our menu items.\n\n\n\n\n\n\n\n\n\nHere we can see the distributions of calories across restaurants. We see that McDonald’s leads with the highest average calories, and it also has several high outliers. Chick Fil-A has the lowest average calories. We will be looking into whether these differences are significant.\n\n\n\n\n\n\n\n\n\nHere, we can take a deeper dive into our data and how the restaurants are divided across protein source. This protein source column was made usings strings to group menu items with various words in their titles. This means that some items, such as those with beef but not burger in the name, were not classified properly and were instead sorted into the other column. We will be focusing on the beef and chicken columns for further analysis."
  },
  {
    "objectID": "SDS272.html#materials-and-methods",
    "href": "SDS272.html#materials-and-methods",
    "title": "McStats: Serving Up the Data on Fast Food Nutrition",
    "section": "Materials and Methods",
    "text": "Materials and Methods\nThe data that we used for this project came from OpenIntro. The dataset came with more variables than we used in the analysis. We decided to remove the vitamin a, vitamin c, calcium, and salad variables, as many of them had several N/A values and we felt that the other variables would be more important to answering our research questions.\nOur first task was to use models to check the relationships between different variables. We built multiple linear regression (MLR) models for predicting calories, amount of protein, and carbohydrates, which were the three variables we decided to use as response variables. Lasso regression was used to select the optimal variables for each of the models. With the lasso models, we ran cross-validation to optimize the penalty. We then ran the model with the optimized penalty to see which variables were most important to predicting the response. Variance inflation factor (VIF) values were also examined to look for and try to reduce any effects of multicollinearity in the models, so we could better understand and interpret the coefficients.\nWe next wanted to look deeper into restaurants, and particularly decided to focus on calorie amounts. An ANOVA test was used to examine the significance of differences between calorie amounts across the various restaurants in the dataset. Further, a Tukey HSD test was used to dig deeper into the ANOVA’s results and determine which restaurants had significantly different mean calorie amounts compared to others.\nNext, logistic regression models were used to find significant predictors of a menu item’s origin (McDonald’s vs. Not McDonald’s) and protein source (Beef vs. Chicken).\nFinally, we wanted to further examine our variables and look into any interactions that could be influencing our models and results, so we tested each of our original multiple linear regressions for interaction terms."
  },
  {
    "objectID": "SDS272.html#results",
    "href": "SDS272.html#results",
    "title": "McStats: Serving Up the Data on Fast Food Nutrition",
    "section": "Results",
    "text": "Results\n\nGeneral Data Information\nThe distribution of menu item calories follows a relatively normal distribution with a little bit of a right skew. The average amount of calories per menu item appears to vary depending on the restaurant, with McDonald’s having the highest and Chick Fil-A having the lowest average. The relationships between calories and each of the nutrients also follow a linear pattern relatively well. We also looked into different protein sources and found that beef and chicken had the most menu items associated with them so we dive deeper into that relationship in the following analysis as well.\n\n\nVariable Selection\nThe Lasso regressions resulted in our three MLR models with calories, protein, and carbohydrates as the response variables. In the regression model for calories we find that total carbs, total fat, and protein are all very significant predictors (p &lt; 2e-16), with trans fat (p = 0.197), cholesterol (p = 0.370), and sodium (p = 0.118) playing a supportive role in prediction. All of the variables except cholesterol were positively correlated with calories with no significant interactions.\nThe adjusted R^2 value for this model is 0.9744, which means that 97.44% of the variability in calories can be accounted for by the variables in the model (p &lt; 2.2e-16).\nThe regression model for total carbs includes significant predictors, calories from fat, fiber, calories (p &lt; 2e-16), protein (p = 3.00e-07), sodium (0.000304), sugar (4.13e-13), and cholesterol (2.51e-07). All of the variables in the model were positively correlated with total carbs except protein, calories from fat, and cholesterol. The adjusted R^2 value for this model is 0.8956 which means that 89.56% of variability in total carbohydrates can be explained by the variables in the model (p &lt; 2.2e-16). The original lasso model included all variables, but multicollinearity issues led us to remove total_fat, trans_fat, and sat_fat.\n\n\n\nThe figure below shows the results of our ANOVA, which we ran to see if the full model was significantly more accurate than our reduced multicollinearity model.\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n492\n32457.07\nNA\nNA\nNA\nNA\n\n\n495\n32521.01\n-3\n-63.93995\n0.3230776\n0.808689\n\n\n\n\n\nIn the regression model for predicting protein we find that calories from fat, sodium, cholesterol, and calories are significant predictors of protein (p &lt; 2e-16), as well as total carb (p = 1.74e-08) and saturated fat (p = 8.44e-07), with fiber playing a supporting role (p = 0.452). We ran into additional multicollinearity issues between variables, so decided to look deeper into potential reduction.\n\n\n\nWe once again perform an ANOVA test with our optimized lasso and our reduced multicollinearity model.\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n495\n12414.37\nNA\nNA\nNA\nNA\n\n\n496\n16185.05\n-1\n-3770.685\n150.3491\n0\n\n\n\n\n\nHere we see a p value of nearly 0, well below our threshold. Because we are trying to find the best model and are not as worried about interpretations of individual variables at this time, we chose to use the larger model, which can more accurately predict protein levels. The adjusted R^2 value for this model is 0.9201. This means that 92.01% of the variability in protein can be explained by the variables in the model (p &lt; 2.2e-16).\n\n\nInteractions\nWe also wanted to look into potential interactions in our model, as we know that often times nutritional values have relationships that might not be obvious.\n\n\n\n\n\nThis matrix of scatterplots shows us the significant interactions in our MLR model for predicting protein.\n\n\n\n\nThere are interactions between calories and total carbs, calories and sodium, total fat and sodium, and sodium and cholesterol. As we can see, higher levels of all of these interaction terms lead to higher slopes of the regression, meaning that they positively affect the relationship.\nCalorie Levels across Restaurants\nThe results of our analysis of variance (ANOVA) for differences in average calories across all restaurants was significant (F-value = 6.085, p = 7.75e-07).\n\n\n\nTo look further into the differences present we performed a Tukey HSD test and found which restaurants had significantly different average calorie amounts.\n\n\n\n\n\n\n\n\n\nRestaurants\nEstimate\nLower Confidence Level\nUpper Confidence Level\nAdjusted P Value\n\n\n\n\nChick Fil-A-Burger King\n-224.1270\n-412.46039\n-35.79357\n0.0077024\n\n\nTaco Bell-Burger King\n-164.9193\n-290.94531\n-38.89320\n0.0019839\n\n\nMcdonalds-Chick Fil-A\n255.9064\n61.68697\n450.12589\n0.0017798\n\n\nSonic-Chick Fil-A\n247.2537\n50.69256\n443.81477\n0.0036044\n\n\nTaco Bell-Mcdonalds\n-196.6987\n-331.36232\n-62.03508\n0.0002868\n\n\nTaco Bell-Sonic\n-188.0459\n-326.06536\n-50.02652\n0.0010247\n\n\n\n\n\nWe found that there are significant differences between Chick Fil-A and Burger King (p = 0.00770), Taco Bell and Burger King (p = 0.00198), McDonald’s and Chick Fil-A (p = 0.00178), Sonic and Chick Fil-A (p = 0.00360), Taco Bell and McDonalds (p = 0.000287), and Taco Bell and Sonic (p = 0.00102). We can be 95% confident that the average calorie difference between Chick Fil-A and Burger King is between -412.46 and -35.79, the difference between Taco Bell and Burger King is between -290.94 and -38.89, the difference between McDonald’s and Chick Fil-A is between 61.69 and 450.13, the difference between Sonic and Chick Fil-A is between 50.69 and 433.81, the difference between Taco Bell and McDonalds is between -331.36 and -62.04, and finally, the difference between Taco Bell and Sonic is between -326.07 and -50.03. We can see that in general, Chick Fil-a and Taco Bell have lower average calorie counts than many of the other restaurants, showing a significant contrast, and McDonalds has a significantly higher calorie count than a few restaurants.\n\nLogistic Regression\nOur last statistical test was logistic regression with binary variables. The first logistic model we created was to calculate the odds of an item being from McDonald’s or not McDonald’s. Our significant predictors were protein (p = 0.000222), fiber (p = 0.000669), and sugar (p = 0.002798). Our findings are presented below:\nFor every 1g increase in protein, the odds that the item is from McDonald’s rises by 13% when all other variables are held constant.\nFor every 1g increase in fiber, the odds the item is from McDonald’s as opposed to other fast food restaurants decreases by about 26.5% when all other variables are held constant.\nFor every 1g increase in sugar, the odds the item is from McDonald’s increase by about 8.6% when all other variables are held constant.\nOur other logistic model was calculating the odds that an item’s protein source was beef vs. chicken. The significant predictors were protein (p = 6.63e-05) and cholesterol (p = 0.000479). We once again present odds ratios for each significant predictor:\nFor every 1g increase in protein, the odds the item is beef rather than chicken decrease by about 12.6% when holding all other variables constant.\nFor every 1g increase in cholesterol, the odds that item is beef rather than chicken increase by about 4% holding all other variables constant."
  },
  {
    "objectID": "SDS272.html#discussion",
    "href": "SDS272.html#discussion",
    "title": "McStats: Serving Up the Data on Fast Food Nutrition",
    "section": "Discussion",
    "text": "Discussion\nCalories are an important statistic to look at when we make decisions about the food we eat, but it is also important to look at the rest of the label to know that our bodies are getting the nutrients that they need to function. Our goals were to find ways to predict calories, protein, and total carbohydrates in fast food meals. The linear regression models that we developed work effectively to do this. These models can help us understand the relationships between different nutrition variables which in turn helps us understand and make better decisions about the meals we eat. Our ANOVA tests can also help people make decisions about which fast food restaurants to eat at based on the average amount of calories in the meals. Our logistic regression models can also be helpful in decision making of menu items and their main protein source.\nThese findings can aid consumers in making healthier and more informed decisions about fast food consumption as we see in the literature that it is not common for people to know what and how much they are eating at fast food restaurants. These results can be generalized to the restaurants in the study, but not outside that scope. Since the restaurants in this study are nationwide chains consumers across the United States can use this information to decide where to eat out.\nOne important limitation of this study is the lack of data for fiber. Since the dataset is already small and some of the values were missing we had to replace the missing values with the mean for the column since it was an important factor. There were also a lot of values missing for the vitamin columns so we had to remove them from the dataset even though they could be valuable predictors.\nSome important steps for future researchers in the field to take are collecting more data. This means less NA values, more restaurants and also more menu items. Something that was disappointing about the dataset we used was that there were no sides, drinks, or deserts represented, especially since it is uncommon to go to fast food restaurants and order only an entrée. There was also no serving sizes, and we noticed that some of our items were way larger portions than others, and very unlikely to be eaten in one sitting. Having nutrition information for servings could be really helpful and a great addition to the dataset. The addition and use of these new data points and variables would be really beneficial for creating even more robust models."
  },
  {
    "objectID": "SDS272.html#appendix",
    "href": "SDS272.html#appendix",
    "title": "McStats: Serving Up the Data on Fast Food Nutrition",
    "section": "Appendix",
    "text": "Appendix\n\n\n\nHere we have a table of all the variables we are using from this dataset. You can see the variable name, type, and whether we are using it as a response or explanatory. Note that for the response variables, when it is not being used as a response it is used as an explanatory variable.\n\n\nVariable\nType\nResponse/Explanatory\n\n\n\n\nrestaurant\ncharacter\nExplanatory\n\n\nitem\ncharacter\nExplanatory\n\n\ncalories\nnumeric\nResponse\n\n\ncal_fat\nnumeric\nExplanatory\n\n\ntotal_fat\nnumeric\nExplanatory\n\n\nsat_fat\nnumeric\nExplanatory\n\n\ntrans_fat\nnumeric\nExplanatory\n\n\ncholesterol\nnumeric\nExplanatory\n\n\nsodium\nnumeric\nExplanatory\n\n\ntotal_carb\nnumeric\nResponse\n\n\nfiber\nnumeric\nExplanatory\n\n\nsugar\nnumeric\nExplanatory\n\n\nprotein\nnumeric\nResponse\n\n\nprotein_source\ncharacter\nExplanatory\n\n\nmcdonalds\nfactor\nExplanatory\n\n\nbeef\nnumeric\nExplanatory\n\n\n\n\n\n\n\n\n\n\nThis graph shows difference in calories between beef and chicken. We see that the average calorie levels are very similar, but chicken does seem to have several outliers. This could be due to the large portion sizes in some of the menu items.\n\n\n\n\n\n\n\n\n\n\n\n\nHere we see the average nutrient amounts by restaurant. This can give us a better idea of which restaurants tend to have the highest or lowest levels for different nutrients.\n\n\n\n\n\n\n\n\n\nHere we see the relationship between a selection of nutrients and calories. All of these have a positive relationship, but some seem to be stronger correlations.\n\n\n\n\n\n\n\n\n\nHere we see the relationship between a selection of nutrients and protein. All of these have a positive relationship, but some seem to be stronger correlations.\n\n\n\n\n\n\n\n\n\nHere we see the relationship between a selection of nutrients and carbohydrates. All of these have a positive relationship, but some seem to be stronger correlations."
  },
  {
    "objectID": "UntitledQMD.html",
    "href": "UntitledQMD.html",
    "title": "Strings & Regular Expressions",
    "section": "",
    "text": "library(readr)\nUntitled_spreadsheet_Sheet1_1_ &lt;- read_csv(\"~/Untitled spreadsheet - Sheet1 (1).csv\")\n\nRows: 4 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Gender\ndbl (1): Number\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\nggplot(data = Untitled_spreadsheet_Sheet1_1_, aes(x = Number, y = Gender)) +\n  geom_boxplot() +\n  scale_x_continuous(limits = c(1, 10)) + # Set x-axis limits from 1 to 10\n  labs(x = \"workplace on a scale of 1-10, 1 being women are favored and 10 being male favored\", \n       y = \"Gender\") # Add a y-axis label"
  },
  {
    "objectID": "ChallengeOne.html",
    "href": "ChallengeOne.html",
    "title": "Final Project - Group B_1_2",
    "section": "",
    "text": "Background Data\nThe hotel dataset was created using data from the hotels’ Property Management System in Portugal. It contains detailed information about hotel reservations, including cancellations, booking dates, the time between booking and arrival, special requests, and more. While this dataset offers numerous opportunities for modeling, our focus is on identifying factors associated with cancellations and predicting when guests are likely to cancel their bookings.\nCleaning Steps\nWe performed several data cleaning steps to improve our models and analysis. For example, we combined the “babies” and “children” columns into a single “children” column and created a “RoomChange” dummy variable to indicate where the requested room differed from the assigned room. We removed variables such as MarketSegment, ArrivalDateYear, ArrivalDateMonth, and ArrivalDateDayOfMonth because their information could be derived from other columns. Similarly, we excluded Country, Agent, and Company due to the large number of specific levels that would not contribute meaningfully to the models. Lastly, we removed columns that contained a lot of missing values.\nQuestion: Can we predict whether a reservation will be cancelled based on booking and guest characteristics?\nVariable\nType\nResponse/Explanatory\nNumber of Levels\n\n\n\n\nIsCanceled\nnumeric\nResponse\nNA\n\n\nLeadTime\nnumeric\nExplanatory\nNA\n\n\nArrivalDateWeekNumber\nnumeric\nExplanatory\nNA\n\n\nStaysInWeekendNights\nnumeric\nExplanatory\nNA\n\n\nStaysInWeekNights\nnumeric\nExplanatory\nNA\n\n\nAdults\nnumeric\nExplanatory\nNA\n\n\nChildren\nnumeric\nExplanatory\nNA\n\n\nMeal\ncharacter\nExplanatory\n5\n\n\nDistributionChannel\ncharacter\nExplanatory\n3\n\n\nIsRepeatedGuest\nnumeric\nExplanatory\nNA\n\n\nPreviousCancellations\nnumeric\nExplanatory\nNA\n\n\nPreviousBookingsNotCanceled\nnumeric\nExplanatory\nNA\n\n\nBookingChanges\nnumeric\nExplanatory\nNA\n\n\nDepositType\ncharacter\nExplanatory\n3\n\n\nDaysInWaitingList\nnumeric\nExplanatory\nNA\n\n\nCustomerType\ncharacter\nExplanatory\n4\n\n\nADR\nnumeric\nExplanatory\nNA\n\n\nRequiredCarParkingSpaces\nnumeric\nExplanatory\nNA\n\n\nTotalOfSpecialRequests\nnumeric\nExplanatory\nNA\n\n\nReservationStatus\ncharacter\nExplanatory\n3\n\n\nRoomChange\nnumeric\nExplanatory\nNA\nTo answer our question, we are using a variable called IsCancelled that is a dummy variable with a value of 0 for not-cancelled bookings and 1 for cancelled bookings. This is our response, and all other variables will be used as explanatory variables.\nVariable\nMin\nMax\nUnique Values\n\n\n\n\nMeal\n2\n9\n5\n\n\nDistributionChannel\n5\n9\n3\n\n\nDepositType\n10\n10\n3\n\n\nCustomerType\n5\n15\n4\n\n\nReservationStatus\n7\n9\n3\n\n\n\n\n\n\n\n\nVariable\nMean\nSD\nMin\nMax\n\n\n\n\nIsCanceled\n0.28\n0.45\n0.00\n1\n\n\nLeadTime\n92.68\n97.29\n0.00\n737\n\n\nArrivalDateWeekNumber\n27.14\n14.01\n1.00\n53\n\n\nStaysInWeekendNights\n1.19\n1.15\n0.00\n19\n\n\nStaysInWeekNights\n3.13\n2.46\n0.00\n50\n\n\nAdults\n1.87\n0.70\n0.00\n55\n\n\nChildren\n0.14\n0.46\n0.00\n10\n\n\nIsRepeatedGuest\n0.04\n0.21\n0.00\n1\n\n\nPreviousCancellations\n0.10\n1.34\n0.00\n26\n\n\nPreviousBookingsNotCanceled\n0.15\n1.00\n0.00\n30\n\n\nBookingChanges\n0.29\n0.73\n0.00\n17\n\n\nDaysInWaitingList\n0.53\n7.43\n0.00\n185\n\n\nADR\n94.95\n61.44\n-6.38\n508\n\n\nRequiredCarParkingSpaces\n0.14\n0.35\n0.00\n8\n\n\nTotalOfSpecialRequests\n0.62\n0.81\n0.00\n5\n\n\nRoomChange\n0.19\n0.39\n0.00\n1\nThis graph shows the number of cancelled reservations in our dataset versus the number of non-cancelled reservations. It is included to show the skew of observations in the dataset we used which informs our decision later on to optimize area under the ROC curve instead of accuracy.\nWe are looking at if lead time (time between booking and check-in) makes a difference to potentially inform our employers if they should set limits to when they open booking. As we can see, there is a wide range of values, and our histogram is extremely right skewed. Most lead times are relatively short, with bookings made closer to the reservation date. However, there are a few notable outliers where reservations were made more than a year in advance.\nThis graph shows how previous cancellations and lead time are correlated with cancellation status. We can see that when the number of previous cancellations is 0, the majority of reservations are not cancelled. However, for higher values of previous cancellations, cancellations become much more common. There does not seem to be strong correlations between cancellation status and lead time, however we do see that all the bookings made more than 500 days in advance were not cancelled\nThese boxplots show us that the average lead time for cancelled reservations is higher than that for not-cancelled reservations. This could have to do with the large influx of reservations with little to no lead time that we saw in the histogram, so if the hotel allows same-day booking those bookings would almost definitely not be cancelled. This graph is interesting and relevant because bookings that occur last minute or close to their time are cancelled less than those further in advance.\nWe can also look into the booking channel. We see that the most cancellations are coming from travel agents/tour operators. This makes sense because they often book in large quantities and might also make provisional bookings for clients who are not yet committed. Furthermore, people are working through a third party which might be easier to cancel than directly through the hotel."
  },
  {
    "objectID": "ChallengeOne.html#auxiliary-functions",
    "href": "ChallengeOne.html#auxiliary-functions",
    "title": "ADM first challenge",
    "section": "",
    "text": "Include and describe the code of the auxiliary functions you will be using. Make sure to include at a minimum the previously defined functions plot_digit(), plot_region(), and calc_prop(), plus any other functions that you will be using\nplot_digit: Converts a row to a vector, to a matrix, allows us to visualize the specific digit\nplot_region: Converts a tbl into a matrix, and allows us to visualize the region\ncalc_prop: Calculates the proportion of dark pixels within the region\n\nregion1 &lt;- read_csv(\"region1.csv\", col_names=FALSE)\n\n\nplot_region &lt;- function(tbl) {\n  digit_mat &lt;- as.matrix(tbl)*128 # Convert tbl into matrix and assign gray=128\n\n  image(t(digit_mat)[,28:1]) #Plot the image making sure is rotated\n}\n\nplot_region(region1)\n\n\n\n\n\n\n\n\n\n# CORRECTED CALC_PROP CODE 9/26\ncalc_prop &lt;- function (region, row) {\n  # Take row from mnist and transform into a \"digit\" matrix\n  digit_mat &lt;-  row |&gt;\n    as.numeric()|&gt;\n    matrix(nrow = 28) |&gt;\n    t()\n  # Find positions of pixels from \"region\"\n  pos = (region==1)\n  # Subset \"digit\" to the positions and count dark pixels (grey&gt;20)\n  dark = digit_mat[pos]&gt;20 \n  # Return proportion of dark pixels of \"image\" in \"region\"\n  return(sum(dark)/sum(pos))\n}"
  },
  {
    "objectID": "ChallengeOne.html#region-description",
    "href": "ChallengeOne.html#region-description",
    "title": "ADM first challenge",
    "section": "Region description",
    "text": "Region description\nFor our first region, we drew two “less than” signs, trying to mimic the number three, hoping that the curves would help differentiate between the numbers. The second design for region two required some trial and error, we ended up making two vertical lines with space in the middle where a three normally go."
  },
  {
    "objectID": "ChallengeOne.html#image-dataset",
    "href": "ChallengeOne.html#image-dataset",
    "title": "ADM first challenge",
    "section": "Image dataset",
    "text": "Image dataset\nCreate your dataset by filtering mnist to include only your two digits of interest and 1000 observations. Save your dataset in your project folder as mnistX_Y.csv (where X and Y are your digits of interest).\n\nmnist1_3_tbl &lt;- read_csv(\"mnist1_3.csv\")\n\n\nregion1 &lt;- read_csv(\"region1.csv\", col_names = FALSE)\nregion2 &lt;- read_csv(\"region2.csv\", col_names = FALSE)\n\n\nplot_region(region1)\n\n\n\n\n\n\n\ncalc_prop(region1, mnist1_3_tbl[1,2:785])\n\n[1] 0.3559322\n\n(tmp_tbl &lt;- mnist1_3_tbl|&gt;\n  rowwise()|&gt;\n  mutate(area=calc_prop(region1,c_across(V1:V784))) |&gt;\n  ungroup()|&gt;\n  select(digit,area))\n\n# A tibble: 100 × 2\n   digit  area\n   &lt;dbl&gt; &lt;dbl&gt;\n 1     1 0.356\n 2     1 0.407\n 3     3 0.746\n 4     1 0.254\n 5     3 0.593\n 6     3 0.712\n 7     1 0.254\n 8     1 0.356\n 9     1 0.322\n10     3 0.831\n# ℹ 90 more rows\n\ntmp_tbl|&gt;\n  ggplot(aes(x=digit, y=area))+\n  geom_boxplot()\n\n\n\n\n\n\n\n\n\nplot_region(region2)\n\n\n\n\n\n\n\ncalc_prop(region2, mnist1_3_tbl[1,2:785])\n\n[1] 0.4444444\n\n(tmp_tbl &lt;- mnist1_3_tbl|&gt;\n  rowwise()|&gt;\n  mutate(area=calc_prop(region2,c_across(V1:V784))) |&gt;\n  ungroup()|&gt;\n  select(digit,area))\n\n# A tibble: 100 × 2\n   digit  area\n   &lt;dbl&gt; &lt;dbl&gt;\n 1     1 0.444\n 2     1 0.889\n 3     3 0.222\n 4     1 0.778\n 5     3 0.667\n 6     3 0.333\n 7     1 1    \n 8     1 0.333\n 9     1 0.778\n10     3 0.556\n# ℹ 90 more rows\n\ntmp_tbl|&gt;\n  ggplot(aes(x=digit, y=area))+\n  geom_boxplot()"
  },
  {
    "objectID": "ChallengeOne.html#feature-dataset",
    "href": "ChallengeOne.html#feature-dataset",
    "title": "ADM first challenge",
    "section": "Feature dataset",
    "text": "Feature dataset\nCalculate the proportion of dark pixels across the two areas of interest (see section The proportion of dark pixels ..) across the dataset from the previous section and create a new tibble features_tbl that contains four columns: id, which is a consecutive number from 1 to 1000, digit which is a factor corresponding to your digit type, prop1 which corresponds to the proportion of dark pixels for region1, and prop2 which is the proportion of dark pixels for region2. Save your dataset in your project folder as mnistX_Y.features.csv (X and Y are your digits of interest).\n\nfeatures_tbl &lt;- mnist1_3_tbl|&gt;\n  rowwise()|&gt;\n  mutate(id = row_number(),\n         prop1 = calc_prop(region1,c_across(V1:V784)),\n         prop2 =  calc_prop(region2,c_across(V1:V784))) |&gt;\n  ungroup()|&gt;\n  select(id, digit,prop1, prop2)\n\n\nfeatures_tbl\n\n# A tibble: 100 × 4\n      id digit prop1 prop2\n   &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1     1     1 0.356 0.444\n 2     1     1 0.407 0.889\n 3     1     3 0.746 0.222\n 4     1     1 0.254 0.778\n 5     1     3 0.593 0.667\n 6     1     3 0.712 0.333\n 7     1     1 0.254 1    \n 8     1     1 0.356 0.333\n 9     1     1 0.322 0.778\n10     1     3 0.831 0.556\n# ℹ 90 more rows\n\n\n\nggplot(features_tbl, aes(x=prop1, y=prop2, group = digit, color = as.factor(digit)))+\n  geom_point()\n\n\n\n\n\n\n\n\nIt looks like the ones are very distinguishable and the threes are also pretty good, although there are a few threes that are in with the ones more and have high prop 2/low prop 1 values."
  },
  {
    "objectID": "ChallengeOne.html#initial-plotting",
    "href": "ChallengeOne.html#initial-plotting",
    "title": "ADM first challenge",
    "section": "Initial plotting",
    "text": "Initial plotting\nGenerate a plot of prop1 versus prop2 and use color to represent the two digit types. Comment on whether the two different digit types are distinguishable using prop1 and prop2."
  },
  {
    "objectID": "ChallengeOne.html#trainingtesting-definition",
    "href": "ChallengeOne.html#trainingtesting-definition",
    "title": "ADM first challenge",
    "section": "Training/Testing definition",
    "text": "Training/Testing definition\nUsing your auatures dataset (see section Feature Dataset) create training/testing datasets. Your training should have 800 observations and your testing should have 200 observations."
  },
  {
    "objectID": "ChallengeOne.html#model-creation-optimization-and-selection",
    "href": "ChallengeOne.html#model-creation-optimization-and-selection",
    "title": "ADM first challenge",
    "section": "Model creation, optimization and selection",
    "text": "Model creation, optimization and selection\nCreate at least two different models for distinguishing the two different digit types using your two defined features. If the models have parameters (for example the k in KNN) make sure to find the optimal values of the parameters. Calculate the accuracy of each model in your dataset, select the model with the highest accuracy, and calculate the confusion matrix for the model with the highest accuracy"
  },
  {
    "objectID": "ChallengeOne.html#decision-boundary",
    "href": "ChallengeOne.html#decision-boundary",
    "title": "ADM first challenge",
    "section": "Decision boundary",
    "text": "Decision boundary\nPlot your model predictions across a grid in prop1 and prop2 using geom_raster()"
  },
  {
    "objectID": "ChallengeOne.html#misclassifications",
    "href": "ChallengeOne.html#misclassifications",
    "title": "ADM first challenge",
    "section": "Misclassifications",
    "text": "Misclassifications\nFor each possible missclassification, plot two of the missclassified digits. What are the values of prop1 and prop2 for these two digits? What is the probability that the model assigns to each of these digits? In your own words explain why the digits get miss-classified."
  },
  {
    "objectID": "ChallengeOne.html#changing-things-up",
    "href": "ChallengeOne.html#changing-things-up",
    "title": "ADM first challenge",
    "section": "Changing things up",
    "text": "Changing things up\n\nPull 500 digit 6s from mnist and add them to your image dataset. Save the dataset in your project folder as mnistX_Y_6.csv (X and Y are your assigned digits)\nCalculate prop_1 and prop_2 on the new previous dataset, and create/save a new dataset as mnist_X_Y_6.features.csv in your project folder. Your dataset should have 4 columns (id, digit, prop1 and prop2)\nSplit the previous dataset into training/testing datasets. Your training should have 1200 observations and your testing should have 300 observations.\nRe-train the model you selected in Model Creation, optimization,... using the new training dataset.\nCalculate the accuracy of your retrained model on the new testing dataset. Calculate the confusion matrix of your model and comment on what digits seem to get confused more and why.\nPlot the decision boundary for your model."
  },
  {
    "objectID": "1326b432541b492ba400b9fd82c2ce1f.html",
    "href": "1326b432541b492ba400b9fd82c2ce1f.html",
    "title": "Mary Kreklow",
    "section": "",
    "text": "ADM first challenge - Final Report\nThis is a kind, inclusive, brave and failure-tolerant class Meghan Walsh, Maya Ballard, Sierra Bouchard, Mary Kreklow 9/24/2024\nAuxiliary functions\nFunction Definitions\nplot_digit: Converts a row to a vector, to a matrix, allows us to visualize the specific digit plot_region: Converts a tbl into a matrix, and allows us to visualize the region\ncalc_prop: Calculates the proportion of dark pixels within the region\n\n\n\n\n\n\n\n\nplot_region &lt;- function(tbl) {\ndigit_mat &lt;- as.matrix(tbl)*128 # Convert tbl into matrix and assign gray=128\n\n\n\n\n\n\n\n\n}\n\nimage( t(digit_mat)[,28:1]) #Plot the image making sure is rotated\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncalc_prop &lt;- function (region, row) {\n# Take row from mnist and transform into a “digit” matrix\ndigit_mat &lt;-row |&gt;\nas.numeric()|&gt;\nmatrix(nrow = 28) |&gt;\nt()\n# Find positions of pixels from “region”\npos = (region==1)\n# Subset “digit” to the positions and count dark pixels (grey&gt;20) dark = digit_mat[pos]&gt;20\n# Return proportion of dark pixels of “image” in “region”\nreturn(sum(dark)/sum(pos))\n}\n\n\n\n\n\nRegion description\nFor our first region, we drew two “less than” signs, trying to mimic the number three, hoping that the curves would help differentiate between the numbers. The second design for region two required some trial and error, we ended up making two vertical lines with space in the middle where a three normally go.\nImage dataset\n\n\n\n\n\n\n\n\nmnis t1_3_tbl&lt;-read_csv(“~/Mscs_341_F24/Class/Data/mnist.csv.gz”) |&gt; filter(digit==1|digit==3)|&gt;\n\n\n\n\n\n\n\n\n\n1\n\n\n\n\n\n\n\nmutate(digit=as.factor(digit)) |&gt;\nslice_head(n=1000)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplot_digit(mnist1_3_tbl[7,])\n\n\n\n\n\n\n\n1.0 0.8 0.6 0.4 0.2 0.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.0 0.2 0.4 0.6 0.8 1.0\n\n\n\n\n\nCode Description\nThe above chunk of code filters the digits our group was assigned (1 and 3). The mutate function converts it into a factor, which allows us to plot it. We plotted the 7th digit in the set using the plot digit function to create a visualization of our assigned digit of 1.\n\n\n\n\n\n\n\n\nregion1 &lt;- read_csv(“~/Mscs_341_F24/Project/B_1/region1.csv”, col_names = FALSE) region2 &lt;- read_csv(“~/Mscs_341_F24/Project/B_1/region2.csv”, col_names = FALSE)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np lot_reg ion(r egion1)\n\n\n\n\n\n\n\n\n\n\n\n\n1.0 0.8 0.6 0.4 0.2 0.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.0\n\n0.2\n0.4\n0.6\n0.8\n\n1.0\n\n\n\n\n2\n\n\n\n\n\n\n\ncalc_prop(region1, mnist1_3_tbl[1,2:785])\n\n\n\n\n\n\n\n\n\n\n## [1] 0.3559322\n\n\n\n\n\n\n\n\ncalc_prop(region1, mnist1_3_tbl[3,2:785])\n\n\n\n\n\n\n\n\n\n\n## [1] 0.7457627\n\n\n\n\n\n\n\n\n\n\n(tmp_tbl &lt;- m nist1_3_tbl|&gt;\nrowwise()|&gt;\n mutate(area=calc_ prop(region1,c_ac ross(V1:V784))) |&gt;\nungroup()|&gt;\n select**(digit,area))\n\n\n\n\n\n\n\n\n## # A tibble: 1,000 x 2\n\n\n\n\n\n##\ndigit\n\narea\n\n\n\n##\n\n&lt;fct&gt; &lt;dbl&gt;\n\n\n\n\n##\n\n1 1\n\n\n0.356\n\n\n\n## ##\n\n2 1\n3 3\n\n\n0.407\n0.746\n\n\n\n##\n\n4 1\n\n\n0.254\n\n\n\n##\n\n5 3\n\n\n0.593\n\n\n\n##\n\n6 3\n\n\n0.712\n\n\n\n##\n\n7 1\n\n\n0.254\n\n\n\n##\n\n8 1\n\n\n0.356\n\n\n\n##\n\n9 1\n\n\n0.322\n\n\n\n\n## 10 3\n\n\n\n0.831\n\n\n\n\n## # i 990 more rows\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ntmp_tbl|&gt;\nggpl ot(aes(x=digit, y=area))+\ngeom_boxplot()\n\n\n\n\n\n\n\narea\n0.75\n0\n5{width=” 1.7777777777777777in” height=“1 .2222222222222223in”}\n5\n\n1 3\ndigit\n\n\n\n\n0.50\n\n\n\n\n0.25\n\n\n\n\n\nCode Description\nThis chunk of code accomplishes several items. First, we plotted our first region. The we calculated the proportion of pixels in our data set that overlapped with the region. We found that the 3 digits overlapped with this region about 75% of the time compared to the ones which overlapped 36% of the time. We then plotted the proportion of 1s and 3s for each regions using box plots.\n\n\n\n\n\n\n\n\nplot_region(region2)\n\n\n\n\n\n\n\n\n\n3\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.0 0.8 0.6 0.4 0.2 0.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeature dataset\n\n\n\n\n\n\n\n\nfeatures_tbl &lt;- mnist1_3_tbl|&gt;\nrowwise()|&gt;\nmutate(\nprop1 = calc_prop(region1,c_across(V1:V784)), prop2 = calc_prop(region2,c_across(V1:V784))) |&gt; ungroup()|&gt;\nmutate(id = row_number())%&gt;%\nselect(id, digit,prop1, prop2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwrite_csv(features_tbl, “~/Mscs_341_F24/Project/B_1/mnist1_3.features.csv”)\n\n\n\n\n\n\n\n\n\n\nCode Description\nIn this chunk we created a tibble with 4 variables: the id, digit, prop 1, and prop 2. Prop1 calculated the proportion of each digit overlap with region one. The same process took place with prop 2, this time with our second region. We then saved the data set.\nInitial plotting\n\n\n\n\n\n\n\n\nggplot(features_tbl, aes(x=prop1, y=prop2, group = digit, color = as.factor(digit)))+ geom_point()+\nlabs(title = “Prop 1 vs. Prop 2”,\nx = “Proportion 1”,\ny = “Proportion 2”,\ncolor = “Digit”)\n\n\n\n\n\n\n\n\n\n5\n\nProp 1 vs. Prop 2\n\n\n\n\nPro portion 2\n1.00\n\n {width= “5.2083 3333333 3333in” he ight=“3 .777777 7777777 777in”}\n\n\n\nDigit\n\n1\n\n\n\n\n\n\n0.75\n\n\n\n\n\n\n\n\n0.50\n\n\n\n! { width=” 0.25in” he ight=“0 .486111 1111111 111in”}\n\n\n\n\n0.25\n\n\n\n\n\n3\n\n\n\n\n0.00\n\n\n\n\n\n\n\n\n\n0.25\n\n0.50\nPro portion 1\n\n\n0.75\n\n\n\n\n\n\n\nIt looks like the ones are very distinguishable and the threes are also pretty good, although there are a few threes that are in with the ones more and have high prop 2/low prop 1 values.\nTraining/Testing Split\n\n\n\n\n\n\n\n\nset.seed(12345)\nfeature_split&lt;- initial_split(features_tbl, prop = 0.8) feature_train &lt;- training(feature_split)\nfeature_test &lt;- testing(feature_split)\n\n\n\n\n\n\n\n\n\n\nSplit our data into testing and training dataset, with a proportion of 80% in our training and 20% in our testing.\nModel creation, optimization and selection - Model 1\n\n\n\n\n\n\n\n\nknn_model&lt;- nearest_neighbor(neighbors = tune())%&gt;%\nset_mode(“classification”)%&gt;%\nset_engine(“kknn”)\nrecipe&lt;- recipe(digit ~ prop1 + prop2, data = feature_train) workflow&lt;- workflow()%&gt;%\nadd_recipe(recipe)%&gt;%\nadd_model(knn_model)\n\n\n\n\n\n\n\n\n\n6\n\n\n\n\n\n\n\n\nlibrary(dials)\nset.seed(12345)\nfeature_fold &lt;- vfold_cv(feature_train, v = 5)\nneighbors_tbl&lt;- grid_regular(neighbors(range = c(1,50)), levels = 50)\nautoplot(tune_neighbor)\n\n\n\n\n\n\n\n\n\ntune_neighbor&lt;- tune_grid(object = workflow, resamples = feature_fold, grid = neighbors_tbl)\naccuracy\n\n0.90\n0.89\n0.88\n0.87\n\nbrier_class\n\n0.12\n0.10\n0.08\n\n\n\n\n\n\n\n\n\n0.96\n\nroc_auc\n\n\n\n\n\n\n\n\n\n\n0.94\n0.92\n0.90\n0.88\n\n\n\n\n\n\n\n\n\n\n\n0\n10\n\n20 30\n# Nearest Neighbors\n\n40\n50\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nshow_best(tune_neighbor, metric = “accuracy”)\n\n\n\n\n\n\n\n\n\n\n## # A tibble: 5 x 7\n## neighbors .metric .estimator ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n## 1\n17 accuracy binary ## 2 18 accuracy binary\n## 3 50 accuracy binary\n## 4 32 accuracy binary\n## 5 33 accuracy binary\nmean n std_err .config\n&lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;\n0.906 5 0.0110 Preprocessor1_Model17\n0.906 5 0.0133 Preprocessor1_Model18\n0.906 5 0.0123 Preprocessor1_Model50\n0.905 5 0.0132 Preprocessor1_Model32\n0.905 5 0.0132 Preprocessor1_Model33\n\n\n\n\n\n\n\n\n\n(best_neighbor&lt;- select_best(tune_neighbor, metric = “accuracy”))\n\n\n\n\n\n\n\n## # A tibble: 1 x 2\n\n\n\n\n\n##\n\n\nneighbors .config\n\n\n\n\n##\n\n\n&lt;int&gt; &lt;chr&gt;\n\n\n\n## 1\n\n17 Preprocessor1_Model17\n\n\n\n\n\n\n\n\n\n\n\nfinal_workflow &lt;- finalize_workflow(workflow, best_neighbor) final_fit &lt;- fit(final_workflow, feature_train)\n\n\n\n\n\n\n\n\n\n7\n\n\n\n\n\n\n\n\n\n\naugme nt(final_fit, feature _test)%&gt;%\na ccuracy(truth = digit, estimate = .pred_class)\n\n\n\n\n\n\n\n\n\n## # A tibble: 1 x 3\n\n\n\n\n\n\n\n##\n\n.metric\n\n.estimator .estimate\n\n\n\n\n\n##\n\n\n&lt;chr&gt;\n\n\n&lt;chr&gt;\n\n\n&lt;dbl&gt;\n\n\n\n\n## 1 accuracy binary\n\n\n\n\n0.92\n\n\n\n\n\nModel 2\n\n\n\n\n\n\n\n\n\n\n\nlog istical_spec&lt;- logistic_r eg()%&gt;%\nset_engine(” glm”)%&gt;%\ns et_mode(“cla ssification”)\n\nworkflow_log&lt;- workfl ow()%&gt;%\nadd_recipe(r ecipe)%&gt;%\nadd_model(lo gistical_spec)\n\nlogistical_fit &lt;- **fit(workflow_log,\nfeature_train) augment( logistical_fit, feature _test)%&gt;%\nac curacy(digit, .pred_class)\n\n\n\n\n\n\n\n\n\n## # A tibble: 1 x 3\n\n\n\n\n\n\n\n##\n\n.metric\n\n.estimator .estimate\n\n\n\n\n\n##\n\n\n&lt;chr&gt;\n\n\n&lt;chr&gt;\n\n\n&lt;dbl&gt;\n\n\n\n\n## 1 accuracy binary\n\n\n\n\n0.93\n\n\n\n\n\n\n\n\n\n\n\n\n\n\naugment( logistical_fit, feature _test)%&gt;%\nco nf_mat(digit, .pred_class)\n\n\n\n\n\n\n\n\n\n##\n\n\nTruth\n\n\n\n\n\n\n## Prediction\n\n\n1\n\n3\n\n\n\n\n##\n\n1 107\n\n\n8\n\n\n\n\n##\n\n3\n6\n\n79\n\n\n\n\n\nCode description\nIn the past two chunks we created our knn model and our multinomial logistical regression model. In the knn model we used cross validation to optimize our k value or our number of neighbors we used. We found that this model had a 92% accuracy. For the multinomial logistic regression model, we got a 93% accuracy, which was slightly better than our knn model. Thus we created a confusion matrix for this model.\nDecision boundary\n\n\n\n\n\n\n\n\nprop_grid&lt;- expand_grid(prop1 = seq(0,1,.01), prop2 = seq(0,1,.01)) augment(logistical_fit, prop_grid)%&gt;%\nggplot(aes(x = prop1, y = prop2, fill = .pred_class))+\ngeom_raster()+\ngeom_point(aes(x = .458, y = .444))+\ngeom_point(aes(x = .356, y = .556))\n\n\n\n\n\n\n\n\n\n8\n\n\n1.00\n0.75\n\n\n\n\n\n\n\n\n\n\nprop2\n\n0.50\n\n.pred_class\n\n\n\n\n\n\n\n\n\n1\n3\n\n\n\n\n\n0.25\n0.00\n\n\n\n\n\n\n\n\n\n\n\n0.00\n0.25\n\n0.50\nprop1\n\n0.75\n1.00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n#we plotted the two misclassifications we were looking at so we could see them better.\n\n\n\n\n\n\n\n\n\n\nCode description\n\nIn this chunk we plotted our logistical fit, our more accurate model, predictions across a grid in prop1 and\n\nprop2 using geom_raster().\nMisclassifications\n\n\n\n\n\n\n\n\n\n\n\n\n\naugment (logist ical_fit, feat ure_test) %&gt;%\nfilte r(digit != .pr ed_class)\n\n\n\n\n\n\n\n\n\n\n\n## # A tibble: 14 x 7\n\n\n\n\n\n\n\n\n##\n\n.p red_class .pred_1 .pred_3\n\n\n\n\nid digit prop1 prop2\n\n\n\n\n##\n\n&lt;fct&gt;\n\n&lt;dbl&gt;\n\n&lt;dbl&gt; &lt;int&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt;\n\n\n\n\n\n##\n\n1 3\n\n0.383\n\n0.617\n\n22 1\n\n0.458 0.444\n\n\n\n##\n\n2 1\n\n0.726\n\n0.274\n\n\n221 3\n\n\n0.407 0.556\n\n\n\n##\n\n3 1\n\n0.553\n\n0.447\n\n\n236 3\n\n\n0.424 0.444\n\n\n\n## ##\n\n4 1\n5 3\n\n\n0.969\n0.154\n\n\n0.0306 0.846\n\n\n237 3 288 1\n\n\n0.305 0.667\n0.559 0.667\n\n\n\n##\n\n6 1\n\n0.789\n\n0.211\n\n\n402 3\n\n\n0.390 0.556\n\n\n\n##\n\n7 1\n\n0.830\n\n0.170\n\n\n501 3\n\n\n0.356 0.444\n\n\n\n##\n\n8 3\n\n0.306\n\n0.694\n\n\n693 1\n\n\n0.475 0.444\n\n\n\n##\n\n9 3\n\n0.401\n\n0.599\n\n\n762 1\n\n\n0.475 0.556\n\n\n\n\n9\n\n\n\n\n\n\n\n\n\n\n\n## 10 1\n\n0.553\n\n0.447\n\n808 3\n\n0.424 0.444\n\n\n\n\n\n\n## 11 3\n\n0.122\n\n0.878\n\n850 1\n\n0.593 0.778\n\n\n\n\n## 12 1\n\n0.808\n\n0.192\n\n870 3\n\n0.322 0.222\n\n\n\n\n## 13 3\n\n0.467\n\n0.533\n\n940 1\n\n0.441 0.444\n\n\n\n\n## 14 1\n\n0.881\n\n0.119\n\n975 3\n\n0.356 0.556\n\n\n\n\n\n\n\n\n\n\n\n\np lot_digit(mnist1_3_tbl[22,])\n\n\n\n\n\n\n\n1.0 0.8 0.6 0.4 0.2 0.0\n\n\n\n\n\n\n\n\n\n0.0 0.2 0.4 0.6 0.8 1.0\n\n\n\n\n\n\n\n\n\n\n\nplot_digit(mnist1_3_tbl[975,])\n\n\n\n\n\n\n\n\n\n10\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.0 0.8 0.6 0.4 0.2 0.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n\n\n\n\nCode Description\nHere we filtered the misclassified digits and selected two of them. We picked id 22 and id 975 as our two misclassified digits. For 22 the value of prop1 was .458 and the value of prop2 was .444. For 975 the value of prop1 was .356 and the value of prop2 was .556. The digits were misclassfied because the proportions were in the decision boundary of the opposite digit. They also were close to the decision boundary. The model gives the probability of our first digit being a 3 a value of 62%. The second digit has a probability of being a 3 of 88%. Looking at the digits, we observed that they were very oddly shaped and did not look like a typical number.\nChanging things up\n\n\n\n\n\n\n\n\n\nmnis t_6tbl&lt;-read_csv(“~/Mscs_3 41_F24/Class/Data/mnist.csv.gz”) |&gt;\nfilter(digit==6)|&gt;\nmut ate(digit=as.factor(digit)) |&gt;\nslice_head(n=500)\nmnist1_3_6_tbl &lt;-mnist_6tbl%&gt;%\nfull_join(mnist1_3_tbl)\n\n\n\n\n\n\nwrite_csv(mnist1_3_6_tbl,\n\n“~/Mscs_341_F 24/Project/B_1/mnist1_3_6.csv”)\n\n\n\n\n\n\n\n\n\n\n\nnew_features_tbl &lt;- mnist1_3_6_tbl|&gt;\nrowwise()|&gt;\nmutate(\n\n\n\n\n\n\n\n\n\n11\n\n\n\n\n\n\n\nprop1 = calc_prop(region1,c_across(V1:V784)), prop2 = calc_prop(region2,c_across(V1:V784))) |&gt; ungroup()|&gt;\nmutate(id = row_number())%&gt;%\nselect(id, digit,prop1, prop2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nwrite_csv(new_features_tbl, “~/Mscs_341_F24/Project/B_1/mnist1_3_6.features.csv”)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nset.seed(12345)\nfeature_6_split &lt;- initial_split(new_features_tbl, prop = .8) feature_6_train &lt;- training(feature_6_split)\nfeature_6_test &lt;- testing(feature_6_split)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnew_log istical_spec&lt;- multinom_r eg()%&gt;%\ns et_engine(“n net”)%&gt;%\ns et_mode(“cla ssification”)\nnew_recipe &lt;- recipe (digit~p rop1+prop2, fe ature_6_train)\nworkflow_log&lt;- workfl ow()%&gt;%\n**add _recipe(new_r ecipe)%&gt;%\nadd _model**(new_lo gistical_spec)\nnew logistical_fit &lt;- fit (workflow_log, fe ature_6_train) augment(new logistical_fit, feature_6\n_test)%&gt;%\nac curacy(digit, .pred_class)\n\n\n\n\n\n\n\n\n\n## # A tibble: 1 x 3\n\n\n\n\n\n\n\n##\n\n.metric\n\n.estimator .estimate\n\n\n\n\n\n##\n\n\n&lt;chr&gt;\n\n\n&lt;chr&gt;\n\n\n&lt;dbl&gt;\n\n\n\n\n## 1 accuracy multiclass\n\n\n\n\n0.74\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\naugment (new_logi stical_fit, fe ature_6_te st)%&gt;% conf_m at(digit, . pred_class)\n\n\n\n\n\n\n\n\n\n\n##\n\n\nTruth\n\n\n\n\n\n\n\n##\nPrediction 6 1\n## 6 68 27\n\n\n\n\n\n3\n1\n\n\n\n\n##\n\n\n1 28 63 11\n\n\n\n\n\n\n\n##\n\n3\n7\n\n4 91\n\n\n\n\n\n\n\n\n\n\n\n\naugment(new_logistical_fit, prop_grid)%&gt;%\nggplot(aes(x = prop1, y = prop2, fill = .pred_class))+ geom_raster()\n\n\n\n\n\n\n\n\n\n12\n\n\n\n\n\n\n\n1.00"
  },
  {
    "objectID": "ChallengeOne.html#lasso-model",
    "href": "ChallengeOne.html#lasso-model",
    "title": "Final Project - Group B_1_2",
    "section": "Lasso Model",
    "text": "Lasso Model\nWe decided to use a lasso model and optimize the penalty to find the best predictors of booking cancellations. We made our training and testing datasets and set up a lasso model with the penalty tuned. We then used cross-validation to find the optimal lambda and added that into our workflow and fit. Our original model contained a variable (reservation status), that had an already known relationship with our response because it said whether the reservations were cancelled or not, so we excluded it from our recipe to fit a better model. We chose to optimize with the roc_auc metric, and used 10 fold cross validation. Because of the extremely skewed nature of our dataset, we chose to find the penalty where specificity and sensitivity were closest to equal. If the hotel predicts too many cancellations that don’t actually happen (false positives), they might double book rooms to compensate. This could lead to the hotel being overfilled and not having enough rooms for all guests. On the other hand, if the hotel fails to predict cancellations (false negatives), they could end up with empty rooms that go unused, losing money as a result. Finding where these are closest to equal can balance those out, helping the hotel combat any issues. We used step_smote in our recipe, which handles class imbalance by generating new examples of the minority class through nearest neighbors. This helped the model equally consider non-cancellations and cancellations, and resulted in a 73.2% model accuracy.\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\naccuracy\nbinary\n0.7315277\n\n\n\n\n\n\n\n\n\n\nThe confusion matrix seen here shows the extra emphasis on the cancelled model to help combat the unbalanced data. True cancellations are predicted correctly about 81.4% of the time, and true non cancellations are predicted correctly about 70% of the time.\n\n\n\n\n\n\n\n\n\nThe five most important variables in this model are seen above. Previous cancellations, transient customer type and non-refundable deposits have a positive relationship with cancellation status, and required car parking spaces and room change have a negative relationship with cancellations."
  },
  {
    "objectID": "ChallengeOne.html#tree-model",
    "href": "ChallengeOne.html#tree-model",
    "title": "Final Project - Group B_1_2",
    "section": "Tree Model",
    "text": "Tree Model\nFor this model, we built a tree and optimized it using bootstrapping, resulting in a final tree with a depth of 15. We used the same recipe as before to ensure that the skewed dataset was being accounted for. This yields about an 78.6% accuracy rate, which is quite a bit better than our lasso model. We chose to optimize using roc_auc because of the imbalanced nature of our dataset, as this metric takes into account both false positives and false negatives.\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\naccuracy\nbinary\n0.7855716\n\n\n\n\n\n\n\n\n\n\nThis model predicts true cancellations about 80.5% correctly and non cancellations about 77.8% correctly.\n\n\n\n\n\n\n\n\n\nThis model has 3 of the same 5 most important variables, room change, required car parking spaces, and customer type, meaning that these most likely have a somewhat strong relationship with cancellations. The other 2 important variables found from the tree model that were not identified as important in the lasso model are lead time and transient-party customer type. Lead time has a positive relationship with cancellations, while transient-party customers have a negative relationship with cancellations."
  },
  {
    "objectID": "ChallengeOne.html#forests",
    "href": "ChallengeOne.html#forests",
    "title": "Final Project - Group B_1_2",
    "section": "Forests",
    "text": "Forests\nOur last model was a random forest, which uses multiple decision trees to optimize accuracy and prevent overfitting.\n\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\naccuracy\nbinary\n0.847978\n\n\n\n\n\n\n\n\n\n\nUsing a random forest, we found a higher accuracy than our other models. The confusion matrix shows improved findings for preventing both type I and II errors. It predicts true cancellations correctly about 75.5% of the time and true non cancellations correctly about 88.4% of the time.\n\n\n\n\n\n\n\n\n\nThree of the top-five important variables seen in the other models, lead time, car parking spaces, and room change, were also here, and this model also includes ADR (average daily revenue of the hotel) and arrival date week (what time of year). ADR has a positive relationship with cancellations, and arrival date week has increased cancellations during the summer/early fall as opposed to other times of the year"
  },
  {
    "objectID": "ChallengeOne.html#conclusion",
    "href": "ChallengeOne.html#conclusion",
    "title": "Final Project - Group B_1_2",
    "section": "Conclusion",
    "text": "Conclusion\nIn conclusion, we found that we can predict with about 86% accuracy whether or not someone will cancel their hotel reservation based on customer characteristics and booking history. Figures summarizing the fit of each of the models and their variable importance are provided and described appropriately above. Some limitations to our finding are that all of our data is from Portugal, meaning that these trends may not be prevalent to other hotels around the world and our scope is limited. Many of our variables are also not normally distributed and are highly skewed, which can affect our models. We did try to combat this, but it is still important to keep in mind.\nWe hope that this modeling could be useful to hotels. Although it is currently limited to select hotels in Portugal, further research could help expand the scope. Ideally, these models could be very helpful for hotels around the world in predicting cancellations based on consumer and reservation characteristics."
  },
  {
    "objectID": "Econ200.html",
    "href": "Econ200.html",
    "title": "Gender Inequalities in Louisiana",
    "section": "",
    "text": "Gender inequalities have persisted for many years, leaving women with far more barriers than men and restricting their access to basic rights. Throughout the years, progress has been made around the world to counteract these inequalities, however, there is still much work to be done. Even developed countries such as the United States face discrimination against women, in some states more severe than others. We chose to study Louisiana because of stories we have heard and its low rankings in terms of gender equality. Several surveys have found that Louisiana is one of the states that is the worst to live in as a woman, and we wanted to further examine why that might be the case. We will explore gender disparities in Louisiana, a state that was once a strong part of the Confederacy. From Louisiana’s toiled past to the present, we will look into demographic gaps, education, workforce participation and pay, and economic outcomes. By examining these facets, we can better understand the scope and impact of gender-based discrimination on women across the state.\nDemographic Gaps\nTo start, we will break down some of the state demographics in Louisiana. As of 2023, 909,420 women between the ages of 15 and 44 resided there. In total, women outnumber the men, with 104 women for every 100 men in the state. This is a higher proportion of women to men than in the country as a whole (102:100).\nfig 1\nAs we can see from the figure above, the distribution of women in Louisiana is quite diverse regarding race/ethnicity, which will be put into perspective when looking into disparities to see if there are discrepancies and racial discrimination alongside gender discrimination. \nFinancially, over 1 in 5 women (20.1%) ages 18-64 had incomes below the federal poverty line in 2019. This is the highest threshold in the country for working-age women. On the other hand, in Louisiana, 14.4% of men in the same age range lived with incomes below the federal poverty line as of 2019. This is a stark 5.7% difference between the two genders. Furthermore, we see that 29.4% of those in poverty were African American, followed closely by 26.1% Latino, 17.1% Native American, and 11.7% Asian American. Only 12.7% of those in poverty in Louisiana were white, even though we saw previously from Figure 1 that over half of the population is white. From this, we can see that intersectionality is a huge factor, as factors like race, age, and geography also play a role in and amplify the inequalities.\nEducation\nWe will next move into education, looking into potential disparities in schooling and what the distributions of men vs. women look like in higher education. Beginning with the enrollment ratio of girls to boys within primary and secondary schools within Lousianna, “School enrollment, primary and secondary (gross), gender parity index (GPI) in the United States was reported at 0.98683% in 2020”. This ratio has become more favorable for females within primary and secondary education since 1999 when the female-to-male ratio was 87 to 100, so much progress has been made. Furthermore, compared to the national average in 2020, which was 97 to 100, Louisiana has reached higher levels of gender parity. \nLooking further into education, we can see where Louisiana stands in higher education, both between men and women and between the state and the US as a whole. The figure below outlines different levels of education and the ratio of males vs. females that are more likely to attain them.\n   fig 2    \nAs seen in Fig. 2, men are quite a bit more likely to have no education, or some but less than high school, than women. Men are astoundingly 44% more likely to have less than a high school education, compared to their female counterparts. This may be due to societal or familial values that lead males to drop out early for work. Males also are much more likely to complete professional programs or doctorates, whereas women are more likely to obtain anything from some college to a master’s. We see this echoed in college enrollment in Louisiana. At Louisiana Colleges, there are 39.19% male students to an overwhelming 60.81% female students. All public institutions in Louisiana saw a decrease of 3.2% in male students (and a corresponding increase of 3.2% for female students) between Fall 2011 and Fall 2020. We could potentially see this upward trend of educated females reflected in other areas, such as economic equality.\nWorkforce Participation and Pay\nLouisiana is one of the worst states in the country for economic equality across genders. As of 2023, women made, on average, 66 cents for every dollar made by a man. Another article breaks it down more and highlights that black women only make 47 cents to each man’s dollar, and Latina women make 53 cents for each man’s dollar. This gap is shown to exist across workplaces and is prevalent in all fields regardless of educational background. It is also seen that the jobs that women more heavily fill tend to be those that pay less than jobs filled primarily by men. \nThis is also reflected in the education field, as Louisiana has significantly more female teachers than males. The average salary for a K-12 educator is around $52,000, which is $12,000 less than the national average teacher salary. Research has shown that about 81% of all teachers are females, which provides strong evidence for the previous claim. \nThere are also significantly fewer female business owners than males. As of the 2017 census, 45,650 males owned firms, whereas women accounted for less than ⅓, which 13,170 firms. This has been progress in this over the years. Louisiana saw an increase from 27.4 to 36.5 percent of female business owners in the last 7 years.\nAn infograph from 2018 states that if trends continue, women in Louisiana will not reach levels of equal pay until the year 2115. This projection was done using data from between 1960-2000, and recent data does not show any major changes or progress, so this is still very much a possibility. The same infograph puts the pay gap into perspective: \n“The difference between women’s and men’s median annual earnings, $15,500, would pay for 4.0 years of community college tuition in Louisiana.”\nHousehold setups like families with single mothers could be severely worse off compared to others.  We can look into household structure to see if it is a good determinant of predicting the gender disparity. The table below shows data from the US Census on different household structures and median income. We can see that the structure with the lowest median income in Louisiana is that of female householders with no spouse present. There is quite a gap between female householders’ income and males’. We can also see the average income of single females with children vs. single males with children, and compare those with the income of married parents with children.\nAs we can see, there is a large difference between the incomes of males and females, regardless of their household structure/status. Single males make almost $17,000 more on average than single females, and males with children make over $24,000 more than females with children. This could be due to several factors, such as women not holding high-level positions in the workplace, or women not being able to secure jobs due to gender-based discrimination.\nWith such a large gap in earnings, we can look into race and ethnicity to see if any discrepancies exist there. The figure below highlights median earnings across different racial groups. With it, we can analyze disparities between men and women, and see if they differ based on race/ethnic groups. \nfig 3\nFrom this figure, we can see the median earnings are lower for women across the board, however, there do not appear to be any groups where it is significantly more prevalent. We do see some variation, leading us to believe that racial discrimination affects both men and women, and doesn’t seem to target women disproportionately. Therefore, for this graph, gender-based factors largely drive this gap and race may not be as big of a factor in earnings.\nConclusion\nIn conclusion, compared to the rest of the nation Louisiana exhibits significant disparities between men and women in terms of demographics, education, workforce, and socioeconomic factors. As seen throughout the paper, Louisiana’s demographics regarding the gender wage gap and unequal treatment within socioeconomic areas suggest the presence of gender discrimination. While there are some redeeming areas in which women are seen to excel, regarding secondary education completion, Louisiana as a whole seems to lag in the push for gender equality in comparison to the progress of the nation as a whole. All of the data we looked into has come out since 2018 and is pretty recent, continuing the notion that although much progress has been made, much more is needed to reach any sort of true gender parity. Furthermore, it confirms our main reasons for choosing Louisianna and solidifies the notion of institutional inequity that women continue to face. \nBibliography\nAnwesha Majumder, Jessica Mason, and National Partnership for Women & Families, “America’s Women and the Wage Gap,” 2024, \nhttps://nationalpartnership.org/wp-content/uploads/2023/02/americas-women-and-the-wage-gap.pdf. \nCollegeTuitionCompare. n.d. “Student Population Comparison Between Louisiana Colleges.” https://www.collegetuitioncompare.com/compare/tables/?state=LA&factor=student-population#%20google_vignette. \nIlluminator, Louisiana. 2023. “Louisiana Teacher Salaries Still Lag Behind Southern, US Average • Louisiana Illuminator.” Louisiana Illuminator. July 5, 2023. https://lailluminator.com/briefs/louisiana-teacher-salaries-still-lag-behind-southern-us-average/. \nIWPR. 2018. “The Economic Status of Women in Louisiana.” Www.Statusofwomendata.Org. https://statusofwomendata.org/wp-content/themes/witsfull/factsheets/economics/factsheet-louisiana.pdf. \nKFF. “State Profiles for Women’s Health | KFF,” November 22, 2024. https://www.kff.org/interactive/womens-health-profiles/louisiana/demographics/. \n“Louisiana Believes - Louisiana Department of Education.” n.d. Louisianabelieves.com. https://louisianabelieves.com. \n“Louisiana Report - 2020 - Talk Poverty.” 2020. Talk Poverty. September 22, 2020. https://talkpoverty.org/state-year-report/louisiana-2020-report/index.html. \nLouisiana Women’s Policy and Research Commission. “Status of Women in Louisiana.” Status of Women in Louisiana. 1st ed., 2020. https://gov.louisiana.gov/assets/Programs/StatusOfWomenInLouisiana_Edition1_OnlineVersion.pdf. \nOffice of Institutional Research. 2021. “Gender Enrollment Trends of Southeastern Louisiana University Fall 2011 – Fall 2020.” Research Brief #35. https://www.southeastern.edu/wp-content/uploads/omni-misc-files/admin/ir/research_briefs/files/ResearchBrief35.pdf. \n“Population Overview.” n.d. March of Dimes | PeriStats. https://www.marchofdimes.org/peristats/data?reg=99&top=14&stop=128&lev=1&slev=4&obj=18&sreg=2%202. \n“The Demographic Statistical Atlas of the United States - Statistical Atlas.” n.d. Statisticalatlas.com. https://statisticalatlas.com/state/Louisiana/Educational-Attainment.\nTRADING ECONOMICS. n.d. “United States - Ratio of Girls to Boys in Primary and Secondary Education - 2024 Data  2025 Forecast 1981-2020 Historical.” https://tradingeconomics.com/united-states/ratio-of-girls-to-boys-in-primary-and-secondary-education-percent-wb-data.html. \nUS Census Bureau. 2023. “2022 ACS 1-year Estimates.” Census.Gov. October 26, 2023. https://www.census.gov/programs-surveys/acs/technical-documentation/table-and-geography-changes/2022/1-year.html."
  }
]