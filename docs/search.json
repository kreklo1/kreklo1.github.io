[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "This is where I’ll put projects from Data Science 2!\n\n\n\n\n\n\n    \n      \n      \n    \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "hw.html",
    "href": "hw.html",
    "title": "HW Assignments",
    "section": "",
    "text": "Maybe I’ll put homework in here–who knows?\n\n\n\n\n\n\n\n    \n      \n      \n    \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mary Kreklow",
    "section": "",
    "text": "Welcome!\nI’m Mary Kreklow, a junior at St. Olaf College, studying quantitative economics with concentrations in statistics & data science and international relations. My academic journey is driven by a commitment to understanding complex systems—whether that’s through data analysis, policy evaluation, or modeling solutions for real-world issues. Outside the classroom, I lead as a SOAR Peer Leader and Senior Admissions Fellow, and I’m actively involved in music ensembles and campus organizations. With a passion for helping others and finding good in the world, I’m excited to work on projects that reflect my dedication to bridging gaps and uplifting communities.\nHere, you’ll find a collection of projects that showcase my curiosity and creativity, offering a playful take on data-driven insights! You can also view my resume here."
  },
  {
    "objectID": "index.html#mary-kreklow",
    "href": "index.html#mary-kreklow",
    "title": "Mary Kreklow",
    "section": "",
    "text": "Welcome!\nI’m Mary Kreklow, a junior at St. Olaf College, studying quantitative economics with concentrations in statistics & data science and international relations. My academic journey is driven by a commitment to understanding complex systems—whether that’s through data analysis, policy evaluation, or modeling solutions for real-world issues. Outside the classroom, I lead as a SOAR Peer Leader and Senior Admissions Fellow, and I’m actively involved in music ensembles and campus organizations. With a passion for helping others and finding good in the world, I’m excited to work on projects that reflect my dedication to bridging gaps and uplifting communities.\nHere, you’ll find a collection of projects that showcase my curiosity and creativity, offering a playful take on data-driven insights! You can also view my resume here."
  },
  {
    "objectID": "us_states.html",
    "href": "us_states.html",
    "title": "US States",
    "section": "",
    "text": "Rat Graph\nThis graph shows the proportion of rats per person for each state. One might use this if they have either a strong like or dislike for rats and are willing to relocate accordingly. Someone who really loves rats and would like to cohabitate with them may wish to check out New York, as we see that there is about a rat for each person residing there. On the other hand, someone who is looking to stay as far away as possible might have better luck with somewhere like Colorado, where they will find closer to one rat for every ten people.",
    "crumbs": [
      "US States"
    ]
  },
  {
    "objectID": "wi_gerrymandering.html",
    "href": "wi_gerrymandering.html",
    "title": "Wisconsin Gerrymandering",
    "section": "",
    "text": "2016 WI Congressional Districts\nThis map shows the congressional Districts in WI as of 2016. The colors from blue to red represent how respectively Democratic or Republican that district voted in the 2016 elections for Congress.\nOne interesting aspect is the shapes of the districts. We see that some districts, like Districts 3 & 4 are very oddly shaped. District 3 seems to reach up into areas like Stevens Point and Eau Claire, while District 4 is by far the smallest, and covers little more than Milwaukee. This could bring up accusations of gerrymandering, especially seeing as the two weirdest shaped districts are also 2 of the 3 that elected Democratic Representatives. If, for example, District 4 was a bit bigger, it might draw some of the republican votes away from their respective districts and make those districts more competitive and at risk of turning blue. The size of District 7 is also notable. It is likely that there are some more liberal areas in NW Wisconsin around Superior, however their votes would be cancelled out by the large span of conservative areas in northcentral WI.\nWe also see that the darkness of the color corresponds to the proportion of votes. For example, districts 3 & 4 have no Republican votes because they were uncontested competitions. The majority of the Republican winning counties had around 60-65% of the vote.\nWI Governor Evers recently signed a bill passing a new Legislative Districts map to combat some of this gerrymandering, however thus far the Congressional Districts have remained the same.",
    "crumbs": [
      "Wisconsin Gerrymandering"
    ]
  },
  {
    "objectID": "rtip.html",
    "href": "rtip.html",
    "title": "R Tip of the Day",
    "section": "",
    "text": "Circlize Package\nThe circlize package is very helpful in displaying data circularly. This can allow us to stack different types of charts about the same data, as shown below, and it also has many benefits pertaining to looking at connections between variables.\n\n\n\n\n\n\n\n\n\nThis next chart is just an example of how you can facet these charts. You can, of course, have multiple circles that all display data and charts, however for this example I choose to just play with their colors and brightness.\n\n\n\n\n\n\n\n\n\nWhile the first two graphs were just playing around with the package with random variables, this last graph is a real world example of how the circlize package might be used. The World Phones dataset shows the amount of phones in different regions of the world throughout a few years. I am using it with the chordDiagram function to show where there were phones in the regions throughout the years.\n# Load circlize package\nlibrary(circlize)\n\n# Load WorldPhones dataset\ndata(\"WorldPhones\")\n\n# Create a circular plot\nchordDiagram(WorldPhones, transparency = 0.5)\n\n\n\n\n\n\n\n\n\nFor more information, this link will take you to a presentation with an overview of the circlize dataset and some of its uses.",
    "crumbs": [
      "R Tip of the Day"
    ]
  },
  {
    "objectID": "mini_project2.html",
    "href": "mini_project2.html",
    "title": "Simulation",
    "section": "",
    "text": "Fat Rats!\n\nFor this project, I examined the dataset FatRats, which has data from an experiment on the effects of diet on the weight gain of rats. The study had 60 baby rat participants, which were split up into groups and fed different sources of protein (Beef, Cereal, and Pork). The resulting dataset shows the weight gain for each rat from their protein source.\nI decided to conduct a randomization-type test by simulating the behavior under the null hypothesis. I wanted to see if the results of weight gain from different protein sources is statistically significant. I did this by running simulations of the study and randomizing the protein source to see if a random test would produce significantly different responses than the experiment did.\nThe following graphs show the difference in average weight gains between different proteins with the randomized test, which shows what might happen if the type of protein had no effect. The hot pink vertical line shows the average weight gain from the study that was run. The p-values below each chart show the probability under the null hypothesis (our randomized experiment) of obtaining a test statistic at least as extreme as the one obtained. The p-values are calculated by looking at how many of the randomized statistics are greater than or equal to the observed statistics from the rat study.\n\n\n\n\n\n\n\n\n\nBeef vs. Pork p-value: 0.298 \n\n\n\n\n\n\n\n\n\n\n\nBeef vs. Cereal p-value: 0.306 \n\n\n\n\n\n\n\n\n\n\n\nPork vs. Cereal p-value: 0.314 \n\n\nAs we can see, all of the p-values are close to 0.3. Generally, if the p-value is less than 0.05, we will consider the results of the study statistically significant. Because these results are well over the point of statistical significance, we can fail to reject the null hypothesis and claim that the difference in weight gains due to different proteins is not statistically significant.",
    "crumbs": [
      "Simulation"
    ]
  },
  {
    "objectID": "RATATOUILLE.html",
    "href": "RATATOUILLE.html",
    "title": "Strings & Regular Expressions",
    "section": "",
    "text": "Ratatouille\nDid you know that in the Ratatouille movie, only 7,526 words were spoken? According to a Reddit post, the average screenplay has about 22,000 words (although Reddit is definitely not the most reliable source so this may not be totally accurate). Within those 7,526 words in Ratatouille, there were 2,193 distinct words spoken. This may seem like very few, but any Ratatouille lover knows how impactful those 2,193 different words were.\n\n\n\n\n\n\n\n\n\nThe above graph is a sentiment analysis of all of the words in Ratatouille. To find this, I used a sentiment library that ranks common words on a scale of -3 to 3, respectively negative to positive sentiment. I inner joined that dataset with my dataset of Ratatouille words to find their sentiments. We can see that a little over half of the Ratatouille words have positive connotations. This makes sense since it is overall a happy movie, although there is a lot of yelling from the head chef which would contribute to the good chunk of negativity. This graph has the words in order of appearance, so the left is the beginning of the movie, and as the movie progresses, we move to the right. We can see areas where the movie was more positive, and also areas that were more negative.\nBelow we see a table with the 10 most used words in Ratatouille. To do this, I removed the stop words (common words that generally don’t hold significance like “the”) from the data and I ranked the leftover words by amount used. Unsurprisingly, we see that 3 of the words have to do with food, as it is a food movie. We also see that names/nicknames come up a lot, with Remy, Gusteau, Linguini and dad all making appearances. Rat is also fairly expected since the whole plot of the movie is about a rat. Somewhat surprising, however, is the word hey. I would think that it would be considered a stop word, however the set of stop words I used did not include it.\n\n\n\n\n\n\nword\nchef\ngusteau\nrat\ncook\ndad\nfood\nhey\nlinguini\nremy\nwait\n\n\nn\n55\n40\n33\n28\n26\n26\n23\n21\n21\n18\n\n\n\n\n\n\n\n\nWhile seeing these words is nice to get a taste for what the most used words are, it is also nice to have a more visual representation. Below is a word cloud containing the top 100 used words from the move. The bigger the word, the more it was used. By using a diagram like this, we can more easily see which words are the most commonly found, and the color codings show up what other words were used a similar amount of times.\n\n\n\n\n\n\n\n\n\nNow that we know which words were used the most in Ratatouille, we can look into where in the movie these words show up. For the next graph, I looked into the top 5 words and where they are positioned in comparison to all other words in the movie. We can see that “dad” was used mostly in 3 separate areas of the movie, whereas “chef” was used pretty regularly throughout.\n\n\n\n\n\n\n\n\n\nAnyone who has watched Ratatouille surely understands that it is a masterpiece, but what do people really think about the film? To find this out, we can look to the master site of movie reviews, Rotten Tomatoes. On Rotten Tomatoes, Ratatouille scores a 96% on the Tomatometer (critic reviews) and an 87% for audience ratings. It is clear that the critics are the true experts in what makes a good movie! The graph below shows a sentiment analysis of words from all reviews (critic and audience).\n\n\n\n\n\n\n\n\n\nWe say that Ratatouille has, by far, had a positive impact on watchers. However, it leaves one to wonder what some of the negative reviews had to say about the movie. The negatively connotated words in these reviews are “cry”, “bored”, “hard”, “struggling”, “critic”, and “leave”. Many of these words are not inherently negative, and would depend on the context.\n\n\n\n\n\n\nword\nreviews\n\n\n\n\ncry\nlike the snooty restaurant &lt;critic&gt; anton ego (voiced by peter o'toole), we enter ratatouille thinking we've tasted it all, and we leave hoping for seconds.\n\n\nbored\nlike the snooty restaurant critic anton ego (voiced by peter o'toole), we enter ratatouille thinking we've tasted it all, and we &lt;leave&gt; hoping for seconds.\n\n\nhard\nlike the snooty restaurant &lt;critic&gt; anton ego (voiced by peter o'toole), we enter ratatouille thinking we've tasted it all, and we leave hoping for seconds.\n\n\nstruggling\nlike the snooty restaurant critic anton ego (voiced by peter o'toole), we enter ratatouille thinking we've tasted it all, and we &lt;leave&gt; hoping for seconds.\n\n\ncritic\nlike the snooty restaurant &lt;critic&gt; anton ego (voiced by peter o'toole), we enter ratatouille thinking we've tasted it all, and we leave hoping for seconds.\n\n\nleave\nlike the snooty restaurant critic anton ego (voiced by peter o'toole), we enter ratatouille thinking we've tasted it all, and we &lt;leave&gt; hoping for seconds.\n\n\n\n\n\n\n\n\nAs we can see, none of these words that are considered to have negative connotations are actually being used in a negative sense. On the contrary, they are actually all part of very positive reviews about the movie.\nNow that we know quite a bit about the contents of Ratatouille, we can look at how some of its statistics compared to other Disney Movies. I know that for a lot of people, time is valuable, so I wanted to see which movies are conserving your time and which are taking more of it. For the first comparison, I found a dataset of many Disney movies and looked at the lengths of the movies.\n\n\n\n\n\n\n\n\n\nWe can see that Ratatouille is just slightly over the average running time, with 111 minutes. The mean running time for the Disney movies in my dataset is 97.51009, with a standard deviation of 18.11338. This means the Ratatouille is within one standard deviation of the mean and is pretty average as far as movie length.\nOne part of Ratatouille that may be less average is its box office statistics. Ratatouille was, and continues to be, very popular, so unsurprisingly it did very well and has a very high box office score of 620.7 million dollars.\n\n\n\n\n\n\n\n\n\nThe average box office for the Disney movies is $169.8 million, however there is a $277.4 million standard deviation, so there is definitely a very wide range. Nonetheless, Ratatouille is still very high on the list and did very well for a Disney movie.\nI was also interested in looking into how the box office scores of Disney movies compared with those of a different company’s movies. I found a dataset of Warner Bros. movies and their box scores to look into the difference between their means.\n\n\n\n\n\n\n\n\n\nWe can see that Warner Bros has more lower revenue movies, while Disney seems to have more higher revenue films. Because we cannot make definite conclusions from this graph, I performed a two-sample t-test to determine if there is a significant difference in the mean box office earnings of Disney movies and Warner Bros. movies.\n\n\n\n    Welch Two Sample t-test\n\ndata:  disney_movies$`Box office (float)` and warner_bros_movies$`Worldwide\\nBox Office`\nt = 3.6751, df = 462.46, p-value = 0.0002656\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 27441671 90514195\nsample estimates:\nmean of x mean of y \n169826342 110848408 \n\n\nWe can see that the results of this t-test find a significant difference in the means, with a p-value in the ten-thousandths. It shows much higher average box office earnings for the Disney movies than the Warner Bros movies, however there are several factors that could be affecting this. Firstly, my dataset for Warner Bros movies is much larger, meaning that it probably includes many smaller films that may have been small outliers and dragged the means down. Furthermore, I also got my dataset for the Disney movies from Kieth Galli’s github and do not have information about how the films were chosen because it is not comprehensive of all Disney movies. One thing that could help find a more accurate result by making the datasets the same size and using the Warner Bros films that have the largest box office earnings.\n\n\n\n    Welch Two Sample t-test\n\ndata:  disney_movies$`Box office (float)` and new_warner$`Worldwide\\nBox Office`\nt = -4.8361, df = 672.97, p-value = 1.643e-06\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -132475481  -55966022\nsample estimates:\nmean of x mean of y \n169826342 264047093 \n\n\nComparing the Disney movies to only the top Warner Bros movies just shows a larger gap, meaning that in reality, Disney movies have had much higher box office earnings and we can reject the null hypothesis and claim a statistical significance between the means. Yay Disney!\nAfter seeing how successful these Disney movies were, we can see if there is any correlation between the amount spent on the movie and how well it did. To do this, I made a scatterplot looking at the relationship between budget and box office earnings.\n\n\n\n\n\n\n\n\n\nWe can see a positive relationship, meaning that the movies with higher budgets tended to do better when they were released. The light pink and slightly larger point represents Ratatouille, so we can see that Ratatouille was very close to the regression line and was following the trend pretty closely.\nKnowing this, I was curious to see what the profits of movies looked like. I made a new variable that subtracted the budgets from the box office earnings to find the profit. I then put the 30 top movies by profit in a table.\n\n\n\n\n\n\ntitle\nBox office (millions)\nBudget (millions)\nProfit (millions)\n\n\n\n\nThe Lion King\n1657.0\n250.0\n1407.0\n\n\nFrozen II\n1450.0\n150.0\n1300.0\n\n\nFrozen\n1280.0\n150.0\n1130.0\n\n\nBeauty and the Beast\n1264.0\n160.0\n1104.0\n\n\nIncredibles 2\n1243.0\n200.0\n1043.0\n\n\nThe Lion King\n968.5\n45.0\n923.5\n\n\nZootopia\n1024.0\n150.0\n874.0\n\n\nToy Story 4\n1073.0\n200.0\n873.0\n\n\nToy Story 3\n1067.0\n200.0\n867.0\n\n\nFinding Dory\n1029.0\n175.0\n854.0\n\n\nFinding Nemo\n940.3\n94.0\n846.3\n\n\nPirates of the Caribbean: Dead Man's Chest\n1066.0\n225.0\n841.0\n\n\nAlice in Wonderland\n1025.0\n200.0\n825.0\n\n\nThe Jungle Book\n966.6\n175.0\n791.6\n\n\nInside Out\n858.0\n175.0\n683.0\n\n\nPirates of the Caribbean: At World's End\n961.0\n300.0\n661.0\n\n\nPirates of the Caribbean: On Stranger Tides\n1046.0\n410.6\n635.4\n\n\nCoco\n807.1\n175.0\n632.1\n\n\nMaleficent\n758.5\n180.0\n578.5\n\n\nThe Chronicles of Narnia: The Lion, the Witch and the Wardrobe\n745.0\n180.0\n565.0\n\n\nPirates of the Caribbean: Dead Men Tell No Tales\n794.9\n230.0\n564.9\n\n\nUp\n735.1\n175.0\n560.1\n\n\nMonsters University\n743.6\n200.0\n543.6\n\n\nThe Incredibles\n633.0\n92.0\n541.0\n\n\nMoana\n690.8\n150.0\n540.8\n\n\nPirates of the Caribbean: The Curse of the Black Pearl\n654.3\n140.0\n514.3\n\n\nBig Hero 6\n657.8\n165.0\n492.8\n\n\nAladdin\n504.1\n28.0\n476.1\n\n\nRatatouille\n620.7\n150.0\n470.7\n\n\nMonsters, Inc.\n577.4\n115.0\n462.4\n\n\n\n\n\n\n\n\nIn this table, Ratatouille takes 29th place as most profitable. Considering this is a dataset of 347 movies, that is quite impressive. It is also worth noting the $470.7 million profit – the masterminds behind Ratatouille must be living well.\nI hope you have enjoyed coming on this data journey with me, and I hope you learned something new (or many new things) about Ratatouille. Now that you have made it to the end, I think that you, dear reader, should have a seat, grab a snack, relax, and enjoy a wonderful screening of Ratatouille!",
    "crumbs": [
      "Strings & Regular Expressions"
    ]
  },
  {
    "objectID": "FinalProject.html",
    "href": "FinalProject.html",
    "title": "Final Project",
    "section": "",
    "text": "Hide-and-SQUEAK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHide-and-SQUEAK\n\n\nLiesl Eckstrom, Maia Chavez, Mary Kreklow\n\n\n12/18/2023\n\n\n\nWant to learn some RAT-ical information to RAT-tle your brain? You have come to the right place! Here, you can learn all about rats in New York City - we will keep no squeak-rets from you! If you have musophobia (a fear of rats) feel free to exit, otherwise join us on our journey as we share and interp-RAT some of our favo-RAT stats. And what better way to showcase this data than in elaboRATe shades of pink?\n\n\n\nIntro to The Rats Dataset\n\n\nFor this project, we focused on using a dataset called “Rat Sightings,” which is from NYC Open Data. This data has service requests of right sightings in New York City from 2010 to the present (Dec. 2023). It offers information about the location of the sighting, including which of New York City’s boroughs, as well as the date and time of the sighting.\n\n\nWe also used a dataset with weather information from NYC. This dataset was scraped from Weather Underground and includes average daily temperatures, daily high and low temperatures, as well as weather events and precipitation details for every day from 1940-2020. We used the data from 2010 to 2020, as these are the dates that overlap with our Rat Sightings dataset.\n\n\nWe will use these data to discover where and when rats are most abundant!\n\n\n\n\nMap\n\n\nMost people are very pro-rat or anti-rat, so they would either be happy to see rats or avoid them at all costs. Because of this, one of the first things that comes to mind when you think of rats is wondering where you are most likely, or least likely, to find them. We decided that the best way to explore this further would be to look at the areas of New York City with the highest concentrations of rats. We decided to make a heat map to help visualization of the most rat-packed areas.\n\n\n\n\n\nAs shown, this is a heat map that shows the areas of New York City with the observations of rat sightings. We decided to use a map of the different boroughs and plot the rat sightings by their longitude and latitude. As shown by the darker shades of pink on the map, the two boroughs with the highest concentration of rat sightings are Brooklyn and Manhattan, followed by Bronx and Queens. If you are a rat-hater, we would recommend sticking close to Staten Island, where there are less rats to be found. However, if you are looking to make some rat friends (perhaps a bro-dent or two), you would probably have the best luck in Brooklyn or Manhattan.\n\n\n\n\nMost Popular Rat Locations\n\n\nIn order to further examine where rats are more abundant, we looked at what types of locations the rats are most commonly sighted. We split the variable Location type into seven categories: residential, other, non residential buildings, outdoor locations, vacant locations, educational locations, and food locations. We decided to use a bar graph to visualize where rats are most commonly seen.\n\n\n\n\n\nIn this bar graph, you can see that most rat sightings occur in residential areas. It’s surprising that there were way more rat sightings in residential areas than there were in outdoor sites, which include places like sewers and parking lots. While this is unsettling, it’s also comforting to know that very few rat sightings occur in food locations such as grocery stores and restaurants. There also weren’t many rat sightings in educational areas. The second most common type of location to see rats was “Other”, which we weren’t given much information about in the dataset, so it is unclear where a lot of rat sightings happen. We can also see from the bar chart that rat sightings in these locations have generally gone up as the years have gone by.\n\n\n\n\nPopular Residential Streets\n\n\nNow that we know that residential areas have the most rat sightings, we can determine which residential streets are most popular with rats. To this we decided to filter the data set to only include residential locations, find the five most frequent streets, and use a faceted scatterplot with the year to see how many rats visit these popular streets each year.\n\n\n\n\n\nThe five most popular residential streets for the rats to hang out in are Bedford Avenue, Broadway, Eastern Parkway, Grand Concourse, and St. Johns Place. So, if you are ever looking to settle down with your rat friends in New York, get a home on these streets. Some of these locations have had an increase in rat friends over the years, but others have a pretty consistent population. As you can see above, St. Johns Place, Bedford Avenue, and Broadway have grown in popularity. St. Johns Place increases a lot, while our Bedford Avenue and Broadway rats increase very slightly. Grand Concourse and Eastern Parkway have been pretty consistent with their number of rat sightings. However, Eastern Parkway did have a very large amount of rat sightings for a couple of years.\n\n\n\n\nRat Sightings in Park Boroughs\n\n\nThe next variables that we looked at with this data set were park borough and month. We made a bar chart with months and rat sightings on the axes to show the spread over the months of the years and used a fill with park borough to show the spread over park borough.\n\n\n\n\n\nThere was a significant trend for more rat sightings in the warmer months than the colder months with a peak in July and steadily decreasing both ways from the peak. We think that this could be for two different reasons, either there are more people out and about to report the rat sightings or there are more rats out and about to be spotted when it’s warmer outside. The park borough with the most amount of rat sightings is Brooklyn and the one with the least amount of sightings is Staten Island. Our hypothesis for this is that Brooklyn has the most rat sightings because it has the highest population and that Staten Island has the least amount of rat sightings for two different reasons. The reasons are that Staten Island has the smallest population and it is more separated from the rest of the New York City park boroughs.\n\n\n\n\nRats and Weather Conditions\n\n\nOne of our other curiosities was if the day’s weather conditions had an effect on the number of rat sightings that were reported. Because our rats dataset didn’t include weather information, we had to add a dataset with weather data to our rats dataset. We joined the datasets together and were able to look at both weather events and daily temperatures. We chose to use the average daily temperature as well as the occurrence of weather events for this graph. We hypothesized that rats would be seen the most on warmer (70s) days with no weather events because that is the time when we most prefer to be out and about and figured the rats might agree.\n\n\n\n\n\nWe were correct in our temperature hypothesis, as we can see that the highest number of rat observations was at temperatures in the upper 70 degrees Fahrenheit. However, we were slightly off with our observation that most of the observations would be on days with no weather events, as it appears that close to half of the observations were made on days with weather events, the main one being rain. This could be for a few reasons, including the fact that most weather events do not last an entire day. Even if a weather event was recorded for the same day as the rat sighting, that does not mean that the rat sighting happened during the weather event. Because we do not have weather observations directly associated with the rat sightings, it is impossible to truly know the distribution. Nonetheless, you do have a greater chance of seeing a rat when there is not a weather event, therefore if you want to avoid rats, you will have a better chance if you go out on miserable days full of snow, thunderstorms, and fog.\n\n\n\n\nLimitations\n\n\nThe largest limitation for our project is that the dataset we used for rat sightings can only track the rats that people are seeing and not all the rats in New York City, so this data is not going to be representative of all the rats in NYC.\n\n\nIt would also be interesting to look into how rat patterns in New York City compare to other cities, and different programs the city might have in place to control the rat population.\n\n\n\n\nConclusion\n\n\nWe hope these rat graphs were not too cheesy for you. Perhaps learning about rats has made you squeak with delight! Either way, we feel that this information is seriously under-rat-ed, and would like to cong-rat-ulate you for making it through. Have a mice day!"
  },
  {
    "objectID": "SDS272.html",
    "href": "SDS272.html",
    "title": "Liesl & Mary EDA",
    "section": "",
    "text": "##Storyboard"
  },
  {
    "objectID": "SDS272.html#storyboard",
    "href": "SDS272.html#storyboard",
    "title": "McStats: Serving Up the Data on Fast Food Nutrition",
    "section": "Storyboard",
    "text": "Storyboard"
  },
  {
    "objectID": "SDS272.html#introductory-graphs",
    "href": "SDS272.html#introductory-graphs",
    "title": "McStats: Serving Up the Data on Fast Food Nutrition",
    "section": "Introductory Graphs",
    "text": "Introductory Graphs"
  },
  {
    "objectID": "SDS272.html#intro",
    "href": "SDS272.html#intro",
    "title": "McStats: Serving Up the Data on Fast Food Nutrition",
    "section": "Intro",
    "text": "Intro\nThe amount of calories in the food that we eat is important. Eating over your maintenance amount of calories can alter the activity of the HPA-axis, which produces hormones, causing our bodies to have higher circulating levels of the stress hormone cortisol (George et al., 2009). Chronic overproduction of cortisol can have many adverse health effects on humans including obesity. Obesity is a rising public health issue, especially in the United States, as the occurrence rates increase (Overweight & Obesity Statistics - Niddk, n.d.). Researching obesity is important because it has high rates of comorbidity with many other very serious health issues including type 2 diabetes, hypertension, sleep apnea, arthritis, and certain types of cancer (Pi-Sunyer, 1999). A factor that can contribute to rising rates of obesity is easy access to affordable, fast, and calorie dense foods.\nAwareness of the caloric content of our food is one important way that we can make informed decisions about the meals that we eat. Fast food chains now, by law, must display the calorie count on each of their items and have nutritional information on hand if requested, but this might not be enough. Block et al., showed that people still underestimate the amount of calories in the fast food meals that they eat, especially when dining at Subway compared to McDonald’s (2013).\nThough eating the right amount of calories is important, it is also not the whole picture. Understanding the nutritional content of the food we eat, such as, levels of protein, carbohydrate, sugar, fat, etc. is invaluable to consider, to make sure that our bodies have all the nutrients they need to function. Knowing these values may also help us make predictions about the amount of calories in our food.\nIn our research we explored different nutritional factors and their relationship with the number of calories, amount of protein, and amount of carbohydrates in fast food meals. We also explored how different fast food chains compare in the amount of calories that are in their products. We hypothesize that there are significant differences between average calorie amounts at different restaurants and that there is a significant linear relationship between our response and explanatory variables."
  },
  {
    "objectID": "SDS272.html#eda",
    "href": "SDS272.html#eda",
    "title": "McStats: Serving Up the Data on Fast Food Nutrition",
    "section": "EDA",
    "text": "EDA\nFirstly, we checked the residuals and conditions of our data to determine if it would be appropriate for modeling because we wanted to run linear regressions. All of our response variables were generally unimodel and roughly normally distributed. The relationship between our response and explanatory variables was linear, as seen in figs. 7-9 in the appendix. We also found equal variance and normalcy in the residuals, aside from some expected high outliers. Because each observation is a menu item, we do not ancipate any issues with independence.\n\n\n\n\n\nThis graph shows the distribution of calories across our menu items.\n\n\n\n\n\n\n\n\n\nHere we can see the distributions of calories across restaurants. We see that McDonald’s leads with the highest average calories, and it also has several high outliers. Chick Fil-A has the lowest average calories. We will be looking into whether these differences are significant.\n\n\n\n\n\n\n\n\n\nHere, we can take a deeper dive into our data and how the restaurants are divided across protein source. This protein source column was made usings strings to group menu items with various words in their titles. This means that some items, such as those with beef but not burger in the name, were not classified properly and were instead sorted into the other column. We will be focusing on the beef and chicken columns for further analysis."
  },
  {
    "objectID": "SDS272.html#materials-and-methods",
    "href": "SDS272.html#materials-and-methods",
    "title": "McStats: Serving Up the Data on Fast Food Nutrition",
    "section": "Materials and Methods",
    "text": "Materials and Methods\nThe data that we used for this project came from OpenIntro. The dataset came with more variables than we used in the analysis. We decided to remove the vitamin a, vitamin c, calcium, and salad variables, as many of them had several N/A values and we felt that the other variables would be more important to answering our research questions.\nOur first task was to use models to check the relationships between different variables. We built multiple linear regression (MLR) models for predicting calories, amount of protein, and carbohydrates, which were the three variables we decided to use as response variables. Lasso regression was used to select the optimal variables for each of the models. With the lasso models, we ran cross-validation to optimize the penalty. We then ran the model with the optimized penalty to see which variables were most important to predicting the response. Variance inflation factor (VIF) values were also examined to look for and try to reduce any effects of multicollinearity in the models, so we could better understand and interpret the coefficients.\nWe next wanted to look deeper into restaurants, and particularly decided to focus on calorie amounts. An ANOVA test was used to examine the significance of differences between calorie amounts across the various restaurants in the dataset. Further, a Tukey HSD test was used to dig deeper into the ANOVA’s results and determine which restaurants had significantly different mean calorie amounts compared to others.\nNext, logistic regression models were used to find significant predictors of a menu item’s origin (McDonald’s vs. Not McDonald’s) and protein source (Beef vs. Chicken).\nFinally, we wanted to further examine our variables and look into any interactions that could be influencing our models and results, so we tested each of our original multiple linear regressions for interaction terms."
  },
  {
    "objectID": "SDS272.html#results",
    "href": "SDS272.html#results",
    "title": "McStats: Serving Up the Data on Fast Food Nutrition",
    "section": "Results",
    "text": "Results\n\nGeneral Data Information\nThe distribution of menu item calories follows a relatively normal distribution with a little bit of a right skew. The average amount of calories per menu item appears to vary depending on the restaurant, with McDonald’s having the highest and Chick Fil-A having the lowest average. The relationships between calories and each of the nutrients also follow a linear pattern relatively well. We also looked into different protein sources and found that beef and chicken had the most menu items associated with them so we dive deeper into that relationship in the following analysis as well.\n\n\nVariable Selection\nThe Lasso regressions resulted in our three MLR models with calories, protein, and carbohydrates as the response variables. In the regression model for calories we find that total carbs, total fat, and protein are all very significant predictors (p &lt; 2e-16), with trans fat (p = 0.197), cholesterol (p = 0.370), and sodium (p = 0.118) playing a supportive role in prediction. All of the variables except cholesterol were positively correlated with calories with no significant interactions.\nThe adjusted R^2 value for this model is 0.9744, which means that 97.44% of the variability in calories can be accounted for by the variables in the model (p &lt; 2.2e-16).\nThe regression model for total carbs includes significant predictors, calories from fat, fiber, calories (p &lt; 2e-16), protein (p = 3.00e-07), sodium (0.000304), sugar (4.13e-13), and cholesterol (2.51e-07). All of the variables in the model were positively correlated with total carbs except protein, calories from fat, and cholesterol. The adjusted R^2 value for this model is 0.8956 which means that 89.56% of variability in total carbohydrates can be explained by the variables in the model (p &lt; 2.2e-16). The original lasso model included all variables, but multicollinearity issues led us to remove total_fat, trans_fat, and sat_fat.\n\n\n\nThe figure below shows the results of our ANOVA, which we ran to see if the full model was significantly more accurate than our reduced multicollinearity model.\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n492\n32457.07\nNA\nNA\nNA\nNA\n\n\n495\n32521.01\n-3\n-63.93995\n0.3230776\n0.808689\n\n\n\n\n\nIn the regression model for predicting protein we find that calories from fat, sodium, cholesterol, and calories are significant predictors of protein (p &lt; 2e-16), as well as total carb (p = 1.74e-08) and saturated fat (p = 8.44e-07), with fiber playing a supporting role (p = 0.452). We ran into additional multicollinearity issues between variables, so decided to look deeper into potential reduction.\n\n\n\nWe once again perform an ANOVA test with our optimized lasso and our reduced multicollinearity model.\n\n\nRes.Df\nRSS\nDf\nSum of Sq\nF\nPr(&gt;F)\n\n\n\n\n495\n12414.37\nNA\nNA\nNA\nNA\n\n\n496\n16185.05\n-1\n-3770.685\n150.3491\n0\n\n\n\n\n\nHere we see a p value of nearly 0, well below our threshold. Because we are trying to find the best model and are not as worried about interpretations of individual variables at this time, we chose to use the larger model, which can more accurately predict protein levels. The adjusted R^2 value for this model is 0.9201. This means that 92.01% of the variability in protein can be explained by the variables in the model (p &lt; 2.2e-16).\n\n\nInteractions\nWe also wanted to look into potential interactions in our model, as we know that often times nutritional values have relationships that might not be obvious.\n\n\n\n\n\nThis matrix of scatterplots shows us the significant interactions in our MLR model for predicting protein.\n\n\n\n\nThere are interactions between calories and total carbs, calories and sodium, total fat and sodium, and sodium and cholesterol. As we can see, higher levels of all of these interaction terms lead to higher slopes of the regression, meaning that they positively affect the relationship.\nCalorie Levels across Restaurants\nThe results of our analysis of variance (ANOVA) for differences in average calories across all restaurants was significant (F-value = 6.085, p = 7.75e-07).\n\n\n\nTo look further into the differences present we performed a Tukey HSD test and found which restaurants had significantly different average calorie amounts.\n\n\n\n\n\n\n\n\n\nRestaurants\nEstimate\nLower Confidence Level\nUpper Confidence Level\nAdjusted P Value\n\n\n\n\nChick Fil-A-Burger King\n-224.1270\n-412.46039\n-35.79357\n0.0077024\n\n\nTaco Bell-Burger King\n-164.9193\n-290.94531\n-38.89320\n0.0019839\n\n\nMcdonalds-Chick Fil-A\n255.9064\n61.68697\n450.12589\n0.0017798\n\n\nSonic-Chick Fil-A\n247.2537\n50.69256\n443.81477\n0.0036044\n\n\nTaco Bell-Mcdonalds\n-196.6987\n-331.36232\n-62.03508\n0.0002868\n\n\nTaco Bell-Sonic\n-188.0459\n-326.06536\n-50.02652\n0.0010247\n\n\n\n\n\nWe found that there are significant differences between Chick Fil-A and Burger King (p = 0.00770), Taco Bell and Burger King (p = 0.00198), McDonald’s and Chick Fil-A (p = 0.00178), Sonic and Chick Fil-A (p = 0.00360), Taco Bell and McDonalds (p = 0.000287), and Taco Bell and Sonic (p = 0.00102). We can be 95% confident that the average calorie difference between Chick Fil-A and Burger King is between -412.46 and -35.79, the difference between Taco Bell and Burger King is between -290.94 and -38.89, the difference between McDonald’s and Chick Fil-A is between 61.69 and 450.13, the difference between Sonic and Chick Fil-A is between 50.69 and 433.81, the difference between Taco Bell and McDonalds is between -331.36 and -62.04, and finally, the difference between Taco Bell and Sonic is between -326.07 and -50.03. We can see that in general, Chick Fil-a and Taco Bell have lower average calorie counts than many of the other restaurants, showing a significant contrast, and McDonalds has a significantly higher calorie count than a few restaurants.\n\nLogistic Regression\nOur last statistical test was logistic regression with binary variables. The first logistic model we created was to calculate the odds of an item being from McDonald’s or not McDonald’s. Our significant predictors were protein (p = 0.000222), fiber (p = 0.000669), and sugar (p = 0.002798). Our findings are presented below:\nFor every 1g increase in protein, the odds that the item is from McDonald’s rises by 13% when all other variables are held constant. \nFor every 1g increase in fiber, the odds the item is from McDonald’s as opposed to other fast food restaurants decreases by about 26.5% when all other variables are held constant. \nFor every 1g increase in sugar, the odds the item is from McDonald’s increase by about 8.6% when all other variables are held constant. \nOur other logistic model was calculating the odds that an item’s protein source was beef vs. chicken. The significant predictors were protein (p = 6.63e-05) and cholesterol (p = 0.000479). We once again present odds ratios for each significant predictor:\nFor every 1g increase in protein, the odds the item is beef rather than chicken decrease by about 12.6% when holding all other variables constant.\nFor every 1g increase in cholesterol, the odds that item is beef rather than chicken increase by about 4% holding all other variables constant."
  },
  {
    "objectID": "SDS272.html#discussion",
    "href": "SDS272.html#discussion",
    "title": "McStats: Serving Up the Data on Fast Food Nutrition",
    "section": "Discussion",
    "text": "Discussion\nCalories are an important statistic to look at when we make decisions about the food we eat, but it is also important to look at the rest of the label to know that our bodies are getting the nutrients that they need to function. Our goals were to find ways to predict calories, protein, and total carbohydrates in fast food meals. The linear regression models that we developed work effectively to do this. These models can help us understand the relationships between different nutrition variables which in turn helps us understand and make better decisions about the meals we eat. Our ANOVA tests can also help people make decisions about which fast food restaurants to eat at based on the average amount of calories in the meals. Our logistic regression models can also be helpful in decision making of menu items and their main protein source.\nThese findings can aid consumers in making healthier and more informed decisions about fast food consumption as we see in the literature that it is not common for people to know what and how much they are eating at fast food restaurants. These results can be generalized to the restaurants in the study, but not outside that scope. Since the restaurants in this study are nationwide chains consumers across the United States can use this information to decide where to eat out.\nOne important limitation of this study is the lack of data for fiber. Since the dataset is already small and some of the values were missing we had to replace the missing values with the mean for the column since it was an important factor. There were also a lot of values missing for the vitamin columns so we had to remove them from the dataset even though they could be valuable predictors.\nSome important steps for future researchers in the field to take are collecting more data. This means less NA values, more restaurants and also more menu items. Something that was disappointing about the dataset we used was that there were no sides, drinks, or deserts represented, especially since it is uncommon to go to fast food restaurants and order only an entrée. There was also no serving sizes, and we noticed that some of our items were way larger portions than others, and very unlikely to be eaten in one sitting. Having nutrition information for servings could be really helpful and a great addition to the dataset. The addition and use of these new data points and variables would be really beneficial for creating even more robust models."
  },
  {
    "objectID": "SDS272.html#appendix",
    "href": "SDS272.html#appendix",
    "title": "McStats: Serving Up the Data on Fast Food Nutrition",
    "section": "Appendix",
    "text": "Appendix\n\n\n\nHere we have a table of all the variables we are using from this dataset. You can see the variable name, type, and whether we are using it as a response or explanatory. Note that for the response variables, when it is not being used as a response it is used as an explanatory variable.\n\n\nVariable\nType\nResponse/Explanatory\n\n\n\n\nrestaurant\ncharacter\nExplanatory\n\n\nitem\ncharacter\nExplanatory\n\n\ncalories\nnumeric\nResponse\n\n\ncal_fat\nnumeric\nExplanatory\n\n\ntotal_fat\nnumeric\nExplanatory\n\n\nsat_fat\nnumeric\nExplanatory\n\n\ntrans_fat\nnumeric\nExplanatory\n\n\ncholesterol\nnumeric\nExplanatory\n\n\nsodium\nnumeric\nExplanatory\n\n\ntotal_carb\nnumeric\nResponse\n\n\nfiber\nnumeric\nExplanatory\n\n\nsugar\nnumeric\nExplanatory\n\n\nprotein\nnumeric\nResponse\n\n\nprotein_source\ncharacter\nExplanatory\n\n\nmcdonalds\nfactor\nExplanatory\n\n\nbeef\nnumeric\nExplanatory\n\n\n\n\n\n\n\n\n\n\nThis graph shows difference in calories between beef and chicken. We see that the average calorie levels are very similar, but chicken does seem to have several outliers. This could be due to the large portion sizes in some of the menu items.\n\n\n\n\n\n\n\n\n\n\n\n\nHere we see the average nutrient amounts by restaurant. This can give us a better idea of which restaurants tend to have the highest or lowest levels for different nutrients.\n\n\n\n\n\n\n\n\n\nHere we see the relationship between a selection of nutrients and calories. All of these have a positive relationship, but some seem to be stronger correlations.\n\n\n\n\n\n\n\n\n\nHere we see the relationship between a selection of nutrients and protein. All of these have a positive relationship, but some seem to be stronger correlations.\n\n\n\n\n\n\n\n\n\nHere we see the relationship between a selection of nutrients and carbohydrates. All of these have a positive relationship, but some seem to be stronger correlations."
  },
  {
    "objectID": "UntitledQMD.html",
    "href": "UntitledQMD.html",
    "title": "Strings & Regular Expressions",
    "section": "",
    "text": "library(readr)\nUntitled_spreadsheet_Sheet1_1_ &lt;- read_csv(\"~/Untitled spreadsheet - Sheet1 (1).csv\")\n\nRows: 4 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Gender\ndbl (1): Number\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\nggplot(data = Untitled_spreadsheet_Sheet1_1_, aes(x = Number, y = Gender)) +\n  geom_boxplot() +\n  scale_x_continuous(limits = c(1, 10)) + # Set x-axis limits from 1 to 10\n  labs(x = \"workplace on a scale of 1-10, 1 being women are favored and 10 being male favored\", \n       y = \"Gender\") # Add a y-axis label"
  }
]